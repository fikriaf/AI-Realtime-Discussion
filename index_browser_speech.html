<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Voice Discussion</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            background: #0a0e27;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
            color: #e0e0e0;
            position: relative;
            overflow: hidden;
        }

        body::before {
            content: '';
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background:
                radial-gradient(circle at 20% 50%, rgba(120, 119, 198, 0.15) 0%, transparent 50%),
                radial-gradient(circle at 80% 80%, rgba(255, 103, 132, 0.15) 0%, transparent 50%),
                radial-gradient(circle at 40% 20%, rgba(138, 43, 226, 0.1) 0%, transparent 50%);
            pointer-events: none;
            z-index: 0;
        }

        .header {
            text-align: center;
            padding: 30px 20px 20px;
            color: #fff;
            position: relative;
            z-index: 1;
        }

        .header h1 {
            font-size: 36px;
            font-weight: 700;
            margin-bottom: 8px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 50%, #f093fb 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .header p {
            font-size: 14px;
            opacity: 0.7;
            color: #a0a0a0;
        }

        .main-container {
            flex: 1;
            display: flex;
            gap: 40px;
            padding: 20px 40px 40px;
            max-width: 1400px;
            width: 100%;
            margin: 0 auto;
            position: relative;
            z-index: 1;
        }

        .ai-panel {
            flex: 1;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            background: rgba(20, 25, 45, 0.6);
            border-radius: 24px;
            padding: 40px;
            box-shadow:
                0 8px 32px rgba(0, 0, 0, 0.4),
                inset 0 1px 0 rgba(255, 255, 255, 0.05);
            position: relative;
            backdrop-filter: blur(10px);
            border: 1px solid rgba(255, 255, 255, 0.05);
        }

        .ai-label {
            position: absolute;
            top: 24px;
            font-size: 24px;
            font-weight: 700;
            color: #888;
            text-transform: uppercase;
            letter-spacing: 2px;
        }

        .circle-container {
            position: relative;
            width: 200px;
            height: 200px;
            margin: 40px 0;
        }

        .circle {
            width: 200px;
            height: 200px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 64px;
            position: relative;
            z-index: 2;
            transition: all 0.3s ease;
        }

        .ai-1 .circle {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            box-shadow:
                0 0 40px rgba(102, 126, 234, 0.4),
                0 0 80px rgba(118, 75, 162, 0.2);
        }

        .ai-2 .circle {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            color: white;
            box-shadow:
                0 0 40px rgba(240, 147, 251, 0.4),
                0 0 80px rgba(245, 87, 108, 0.2);
        }

        .ripple {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            width: 200px;
            height: 200px;
            border-radius: 50%;
            border: 3px solid;
            opacity: 0;
            z-index: 1;
        }

        .ai-1 .ripple {
            border-color: #667eea;
            box-shadow: 0 0 20px rgba(102, 126, 234, 0.6);
        }

        .ai-2 .ripple {
            border-color: #f5576c;
            box-shadow: 0 0 20px rgba(245, 87, 108, 0.6);
        }

        .circle-container.active .ripple {
            animation: ripple 1.5s ease-out infinite;
        }

        @keyframes ripple {
            0% {
                width: 200px;
                height: 200px;
                opacity: 0.6;
            }

            100% {
                width: 300px;
                height: 300px;
                opacity: 0;
            }
        }

        .text-display {
            min-height: 120px;
            max-height: 200px;
            overflow-y: auto;
            text-align: center;
            font-size: 16px;
            line-height: 1.8;
            color: #e0e0e0;
            padding: 24px;
            background: rgba(10, 14, 39, 0.5);
            border-radius: 16px;
            width: 100%;
            border: 1px solid rgba(255, 255, 255, 0.05);
            box-shadow: inset 0 2px 10px rgba(0, 0, 0, 0.3);
        }

        .word-fade {
            animation: fadeInWord 0.3s ease-out;
            display: inline-block;
        }

        @keyframes fadeInWord {
            0% {
                opacity: 0;
            }

            100% {
                opacity: 1;
            }
        }

        .text-display.empty {
            color: #555;
            font-style: italic;
        }

        .text-display::-webkit-scrollbar {
            width: 6px;
        }

        .text-display::-webkit-scrollbar-track {
            background: rgba(0, 0, 0, 0.2);
            border-radius: 3px;
        }

        .text-display::-webkit-scrollbar-thumb {
            background: rgba(255, 255, 255, 0.1);
            border-radius: 3px;
        }

        .text-display::-webkit-scrollbar-thumb:hover {
            background: rgba(255, 255, 255, 0.2);
        }

        .controls {
            position: fixed;
            bottom: 30px;
            left: 50%;
            transform: translateX(-50%);
            display: flex;
            gap: 12px;
            background: rgba(20, 25, 45, 0.8);
            padding: 16px 24px;
            border-radius: 50px;
            box-shadow:
                0 8px 32px rgba(0, 0, 0, 0.5),
                inset 0 1px 0 rgba(255, 255, 255, 0.1);
            z-index: 100;
            backdrop-filter: blur(20px);
            border: 1px solid rgba(255, 255, 255, 0.1);
        }

        .btn {
            padding: 12px 28px;
            border: none;
            border-radius: 24px;
            font-size: 13px;
            font-weight: 700;
            cursor: pointer;
            transition: all 0.3s ease;
            text-transform: uppercase;
            letter-spacing: 1px;
            position: relative;
            overflow: hidden;
        }

        .btn::before {
            content: '';
            position: absolute;
            top: 50%;
            left: 50%;
            width: 0;
            height: 0;
            border-radius: 50%;
            background: rgba(255, 255, 255, 0.2);
            transform: translate(-50%, -50%);
            transition: width 0.6s, height 0.6s;
        }

        .btn:hover::before {
            width: 300px;
            height: 300px;
        }

        .btn:disabled {
            opacity: 0.4;
            cursor: not-allowed;
        }

        .btn-start {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3);
        }

        .btn-start:hover:not(:disabled) {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(102, 126, 234, 0.5);
        }

        .btn-stop {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            color: white;
            box-shadow: 0 4px 15px rgba(245, 87, 108, 0.3);
        }

        .btn-stop:hover:not(:disabled) {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(245, 87, 108, 0.5);
        }

        .btn-clear {
            background: rgba(255, 255, 255, 0.05);
            color: #aaa;
            border: 1px solid rgba(255, 255, 255, 0.1);
        }

        .btn-clear:hover {
            background: rgba(255, 255, 255, 0.1);
            color: #fff;
        }

        .status-bar {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            background: rgba(10, 14, 39, 0.95);
            color: white;
            padding: 10px 16px;
            text-align: center;
            font-size: 13px;
            z-index: 200;
            backdrop-filter: blur(20px);
            border-bottom: 1px solid rgba(255, 255, 255, 0.05);
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.3);
        }

        .status-bar.hidden {
            display: none;
        }

        @media (max-width: 968px) {
            .header h1 {
                font-size: 28px;
            }

            .main-container {
                flex-direction: column;
                gap: 20px;
                padding: 20px;
            }

            .ai-panel {
                padding: 30px 20px;
            }

            .circle-container {
                width: 150px;
                height: 150px;
            }

            .circle {
                width: 150px;
                height: 150px;
                font-size: 48px;
            }

            .ripple {
                width: 150px;
                height: 150px;
            }

            .text-display {
                min-height: 100px;
                max-height: 150px;
                font-size: 14px;
                padding: 16px;
            }

            .controls {
                bottom: 20px;
                padding: 12px 16px;
                gap: 8px;
            }

            .btn {
                padding: 10px 20px;
                font-size: 12px;
            }

            @keyframes ripple {
                0% {
                    width: 150px;
                    height: 150px;
                    opacity: 0.6;
                }

                100% {
                    width: 250px;
                    height: 250px;
                    opacity: 0;
                }
            }
        }

        /* Configuration Panel */
        .config-panel {
            position: fixed;
            bottom: 0;
            left: 0;
            right: 0;
            background: rgba(20, 25, 45, 0.98);
            backdrop-filter: blur(20px);
            border-top: 1px solid rgba(255, 255, 255, 0.1);
            box-shadow: 0 -8px 32px rgba(0, 0, 0, 0.5);
            z-index: 150;
            max-height: 60vh;
            overflow-y: auto;
        }

        .config-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 20px 40px;
            border-bottom: 1px solid rgba(255, 255, 255, 0.05);
        }

        .config-header h3 {
            color: #fff;
            font-size: 20px;
            margin: 0;
        }

        .btn-close {
            background: rgba(255, 255, 255, 0.05);
            border: none;
            color: #fff;
            width: 32px;
            height: 32px;
            border-radius: 50%;
            cursor: pointer;
            font-size: 18px;
            transition: all 0.3s;
        }

        .btn-close:hover {
            background: rgba(255, 255, 255, 0.1);
            transform: rotate(90deg);
        }

        .config-content {
            padding: 20px 40px 40px;
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 30px;
        }

        .config-section {
            background: rgba(255, 255, 255, 0.02);
            padding: 20px;
            border-radius: 12px;
            border: 1px solid rgba(255, 255, 255, 0.05);
        }

        .config-section h4 {
            color: #667eea;
            font-size: 16px;
            margin: 0 0 20px 0;
        }

        .config-item {
            margin-bottom: 20px;
        }

        .config-item:last-child {
            margin-bottom: 0;
        }

        .config-item label {
            display: block;
            color: #aaa;
            font-size: 13px;
            margin-bottom: 8px;
        }

        .config-item label span {
            color: #667eea;
            font-weight: 600;
        }

        .config-select {
            width: 100%;
            padding: 10px;
            background: rgba(255, 255, 255, 0.05);
            border: 1px solid rgba(255, 255, 255, 0.1);
            border-radius: 8px;
            color: #fff;
            font-size: 14px;
            cursor: pointer;
        }

        .config-select option {
            background: #1a1f3a;
            color: #fff;
        }

        .config-slider {
            width: 100%;
            height: 6px;
            border-radius: 3px;
            background: rgba(255, 255, 255, 0.1);
            outline: none;
            cursor: pointer;
        }

        .config-slider::-webkit-slider-thumb {
            appearance: none;
            width: 18px;
            height: 18px;
            border-radius: 50%;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            cursor: pointer;
            box-shadow: 0 2px 8px rgba(102, 126, 234, 0.5);
        }

        .config-slider::-moz-range-thumb {
            width: 18px;
            height: 18px;
            border-radius: 50%;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            cursor: pointer;
            border: none;
            box-shadow: 0 2px 8px rgba(102, 126, 234, 0.5);
        }

        .config-actions {
            grid-column: 1 / -1;
            display: flex;
            gap: 12px;
            justify-content: center;
            margin-top: 20px;
        }

        @media (max-width: 968px) {
            .config-content {
                grid-template-columns: 1fr;
                padding: 20px;
            }

            .config-header {
                padding: 15px 20px;
            }
        }
    </style>
</head>

<body>
    <div class="status-bar" id="statusBar">Ready</div>

    <div class="header">
        <h1>AI Voice Discussion</h1>
        <p>Browser Speech Recognition ‚Ä¢ Real-time Conversation</p>
    </div>

    <div class="main-container">
        <!-- AI-1: User Input (Left) -->
        <div class="ai-panel ai-1">
            <div class="ai-label">AI-1</div>
            <div class="circle-container" id="ai1Circle">
                <div class="circle">ü§ñ</div>
                <div class="ripple"></div>
                <div class="ripple" style="animation-delay: 0.5s;"></div>
            </div>
            <div class="text-display empty" id="userTranscript">Waiting for input...</div>
        </div>

        <!-- AI-2: AI Response (Right) -->
        <div class="ai-panel ai-2">
            <div class="ai-label">AI-2</div>
            <div class="circle-container" id="ai2Circle">
                <div class="circle">ü§ñ</div>
                <div class="ripple"></div>
                <div class="ripple" style="animation-delay: 0.5s;"></div>
            </div>
            <div class="text-display empty" id="aiResponse">Waiting for your message...</div>
        </div>
    </div>

    <div class="controls">
        <button id="startBtn" class="btn btn-start">Start</button>
        <button id="stopBtn" class="btn btn-stop" disabled>Stop</button>
        <button id="clearBtn" class="btn btn-clear">Clear</button>
        <button id="configBtn" class="btn btn-clear">‚öôÔ∏è Config</button>
    </div>

    <!-- Configuration Panel -->
    <div class="config-panel" id="configPanel" style="display: none;">
        <div class="config-header">
            <h3>‚öôÔ∏è Configuration</h3>
            <button id="closeConfigBtn" class="btn-close">‚úï</button>
        </div>

        <div class="config-content">
            <!-- Voice Settings -->
            <div class="config-section">
                <h4>üé§ Voice Settings</h4>
                <div class="config-item">
                    <label for="voiceSelect">Voice:</label>
                    <select id="voiceSelect" class="config-select"></select>
                </div>
                <div class="config-item">
                    <label for="rateSlider">Speed: <span id="rateValue">1.2</span>x</label>
                    <input type="range" id="rateSlider" min="0.5" max="2.0" step="0.1" value="1.2"
                        class="config-slider">
                </div>
                <div class="config-item">
                    <label for="pitchSlider">Pitch: <span id="pitchValue">1.3</span></label>
                    <input type="range" id="pitchSlider" min="0.0" max="2.0" step="0.1" value="1.3"
                        class="config-slider">
                </div>
                <div class="config-item">
                    <label for="volumeSlider">Volume: <span id="volumeValue">1.0</span></label>
                    <input type="range" id="volumeSlider" min="0.0" max="1.0" step="0.1" value="1.0"
                        class="config-slider">
                </div>
            </div>

            <!-- Recognition Settings -->
            <div class="config-section">
                <h4>üéôÔ∏è Recognition Settings</h4>
                <div class="config-item">
                    <label for="recognitionConfidenceInput">Recognition Confidence: <span
                            id="recognitionConfidenceValue">0.3</span></label>
                    <input type="range" id="recognitionConfidenceInput" min="0.0" max="1.0" step="0.05" value="0.3"
                        class="config-slider">
                    <small style="color: #888; font-size: 11px;">Lower = more sensitive | 0.3 = ULTRA SENSITIVE</small>
                </div>
                <div class="config-item">
                    <label for="interruptConfidenceInput">Interrupt Confidence: <span
                            id="interruptConfidenceValue">0.2</span></label>
                    <input type="range" id="interruptConfidenceInput" min="0.0" max="1.0" step="0.05" value="0.2"
                        class="config-slider">
                    <small style="color: #888; font-size: 11px;">Higher = less false interrupts | 0.1 = HYPER
                        SENSITIVE</small>
                </div>
                <div class="config-item">
                    <label for="sendDelayInput">Send Delay (ms): <span id="sendDelayValue">600</span></label>
                    <input type="range" id="sendDelayInput" min="100" max="2000" step="100" value="600"
                        class="config-slider">
                </div>
                <div class="config-item">
                    <label for="maxWaitInput">Max Wait Time (ms): <span id="maxWaitValue">2000</span></label>
                    <input type="range" id="maxWaitInput" min="1000" max="10000" step="1000" value="2000"
                        class="config-slider">
                </div>
                <div class="config-item">
                    <label for="minWordsInput">Min Words to Send: <span id="minWordsValue">1</span></label>
                    <input type="range" id="minWordsInput" min="1" max="5" step="1" value="1" class="config-slider">
                    <small style="color: #888; font-size: 11px;">1 = Allow single words like "yes", "no", "stop"</small>
                </div>
            </div>

            <!-- Echo Filter Settings -->
            <div class="config-section">
                <h4>üîá Echo Filter Settings</h4>
                <div class="config-item">
                    <label for="echoThresholdInput">Echo Threshold (%): <span id="echoThresholdValue">20</span></label>
                    <input type="range" id="echoThresholdInput" min="10" max="50" step="5" value="20"
                        class="config-slider">
                </div>
                <div class="config-item">
                    <label for="maxTokensInput">Max Tokens: <span id="maxTokensValue">50</span></label>
                    <input type="range" id="maxTokensInput" min="50" max="200" step="10" value="50"
                        class="config-slider">
                </div>
            </div>

            <div class="config-actions">
                <button id="testVoiceBtn" class="btn btn-start">üîä Test Voice</button>
                <button id="resetConfigBtn" class="btn btn-clear">‚Ü∫ Reset to Default</button>
            </div>
        </div>
    </div>

    <input type="hidden" id="backendUrl" value="http://localhost:8000" />

    <script>
        // Debug flags
        const DEBUG_RECOGNITION = false; // Recognition events
        const DEBUG_ACCUMULATION = false; // Text accumulation
        const DEBUG_TIMERS = false; // Send timers
        const DEBUG_ECHO = false; // Echo filtering
        const DEBUG_INTERRUPT = false; // Interrupt detection
        const DEBUG_DISPLAY = false; // UI updates
        const DEBUG_API = true; // API calls (KEEP ON)
        const DEBUG_RESET = false; // Recognition reset (OFF)

        // Global state
        let isListening = false;
        let recognition = null;
        let recognitionAI2 = null; // Recognition khusus untuk AI-2
        let synthesis = window.speechSynthesis;
        let conversationHistory = [];
        let isSpeaking = false;
        const MAX_HISTORY = 20;
        let accumulatedTranscript = '';
        let lastSendTime = 0;
        // NOTE: These constants are now controlled by config UI
        // Use SEND_DELAY_CONFIG, MAX_WAIT_TIME_CONFIG, etc. instead
        const MIN_WORDS_TO_SPEAK = 3; // Minimum words before AI speaks (prevent noise response)
        let firstSpeechTime = 0; // Track when user started speaking
        let originalAIText = ''; // Teks asli dari AI untuk validasi
        let sendTimeoutId = null; // Track timeout for cancellation
        let isProcessing = false; // Prevent multiple simultaneous sends
        let restartTimeoutId = null; // Track restart timeout
        let sendQueue = []; // Queue for pending sends
        let isSending = false; // Track if currently sending
        let ai2CurrentText = ''; // Track AI-2 output for echo detection
        let ai2ResponseHistory = []; // Track last 3 AI-2 responses for extended echo filtering
        const MAX_AI2_HISTORY = 3; // Keep last 3 responses
        let lastInterruptTime = 0; // Prevent interrupt spam
        let interruptCooldown = 500; // Min 500ms between interrupts
        let isRestarting = false; // Prevent restart loop
        let restartingSafetyTimeout = null; // Safety timeout to force reset isRestarting
        let displayedAI2Words = []; // Track all displayed AI-2 words for animation accumulation
        let shouldStopAfterSentence = false; // Flag for graceful interrupt at sentence boundary
        let currentUtterance = null; // Track current speaking utterance
        let recognitionHeartbeat = null; // Heartbeat to check if recognition is alive
        let lastUserInputTime = Date.now(); // Track last user input
        let silenceNudgeTimer = null; // Timer for sending nudge after silence
        const SILENCE_NUDGE_DELAY = 10000; // 10 seconds of silence before nudge

        // DOM elements
        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const clearBtn = document.getElementById('clearBtn');
        const statusBar = document.getElementById('statusBar');
        const userTranscriptEl = document.getElementById('userTranscript');
        const aiResponseEl = document.getElementById('aiResponse');
        const ai1Circle = document.getElementById('ai1Circle');
        const ai2Circle = document.getElementById('ai2Circle');
        const backendUrlInput = document.getElementById('backendUrl');

        // Config elements
        const configBtn = document.getElementById('configBtn');
        const configPanel = document.getElementById('configPanel');
        const closeConfigBtn = document.getElementById('closeConfigBtn');
        const voiceSelect = document.getElementById('voiceSelect');
        const rateSlider = document.getElementById('rateSlider');
        const pitchSlider = document.getElementById('pitchSlider');
        const volumeSlider = document.getElementById('volumeSlider');
        const sendDelayInput = document.getElementById('sendDelayInput');
        const maxWaitInput = document.getElementById('maxWaitInput');
        const minWordsInput = document.getElementById('minWordsInput');
        const echoThresholdInput = document.getElementById('echoThresholdInput');
        const maxTokensInput = document.getElementById('maxTokensInput');
        const recognitionConfidenceInput = document.getElementById('recognitionConfidenceInput');
        const interruptConfidenceInput = document.getElementById('interruptConfidenceInput');
        const testVoiceBtn = document.getElementById('testVoiceBtn');
        const resetConfigBtn = document.getElementById('resetConfigBtn');

        // Config variables (will be updated from sliders)
        let selectedVoice = null;
        let voiceRate = 1.2;
        let voicePitch = 1.3;
        let voiceVolume = 1.0;
        let SEND_DELAY_CONFIG = 600;
        let MAX_WAIT_TIME_CONFIG = 2000;
        let MIN_WORDS_TO_SEND_CONFIG = 1; // Allow single words for natural conversation
        let ECHO_THRESHOLD_CONFIG = 20;
        let MAX_TOKENS_CONFIG = 50;
        let RECOGNITION_CONFIDENCE_CONFIG = 0.3; // Minimum confidence to accept transcript (ULTRA SENSITIVE)
        let INTERRUPT_CONFIDENCE_CONFIG = 0.2;   // Minimum confidence to trigger interrupt (HYPER SENSITIVE)

        // Event listeners
        startBtn.addEventListener('click', startListening);
        stopBtn.addEventListener('click', stopListening);
        clearBtn.addEventListener('click', clearHistory);

        // Config panel listeners
        configBtn.addEventListener('click', () => {
            configPanel.style.display = configPanel.style.display === 'none' ? 'block' : 'none';
        });

        closeConfigBtn.addEventListener('click', () => {
            configPanel.style.display = 'none';
        });

        // Voice selection
        voiceSelect.addEventListener('change', (e) => {
            const voices = synthesis.getVoices();
            selectedVoice = voices[e.target.selectedIndex];
            console.log('Selected voice:', selectedVoice.name);
        });

        // Sliders
        rateSlider.addEventListener('input', (e) => {
            voiceRate = parseFloat(e.target.value);
            document.getElementById('rateValue').textContent = voiceRate.toFixed(1);
        });

        pitchSlider.addEventListener('input', (e) => {
            voicePitch = parseFloat(e.target.value);
            document.getElementById('pitchValue').textContent = voicePitch.toFixed(1);
        });

        volumeSlider.addEventListener('input', (e) => {
            voiceVolume = parseFloat(e.target.value);
            document.getElementById('volumeValue').textContent = voiceVolume.toFixed(1);
        });

        sendDelayInput.addEventListener('input', (e) => {
            SEND_DELAY_CONFIG = parseInt(e.target.value);
            document.getElementById('sendDelayValue').textContent = SEND_DELAY_CONFIG;
        });

        maxWaitInput.addEventListener('input', (e) => {
            MAX_WAIT_TIME_CONFIG = parseInt(e.target.value);
            document.getElementById('maxWaitValue').textContent = MAX_WAIT_TIME_CONFIG;
        });

        minWordsInput.addEventListener('input', (e) => {
            MIN_WORDS_TO_SEND_CONFIG = parseInt(e.target.value);
            document.getElementById('minWordsValue').textContent = MIN_WORDS_TO_SEND_CONFIG;
        });

        echoThresholdInput.addEventListener('input', (e) => {
            ECHO_THRESHOLD_CONFIG = parseInt(e.target.value);
            document.getElementById('echoThresholdValue').textContent = ECHO_THRESHOLD_CONFIG;
        });

        maxTokensInput.addEventListener('input', (e) => {
            MAX_TOKENS_CONFIG = parseInt(e.target.value);
            document.getElementById('maxTokensValue').textContent = MAX_TOKENS_CONFIG;
        });

        recognitionConfidenceInput.addEventListener('input', (e) => {
            RECOGNITION_CONFIDENCE_CONFIG = parseFloat(e.target.value);
            document.getElementById('recognitionConfidenceValue').textContent = RECOGNITION_CONFIDENCE_CONFIG.toFixed(2);
        });

        interruptConfidenceInput.addEventListener('input', (e) => {
            INTERRUPT_CONFIDENCE_CONFIG = parseFloat(e.target.value);
            document.getElementById('interruptConfidenceValue').textContent = INTERRUPT_CONFIDENCE_CONFIG.toFixed(2);
        });

        // Test voice button
        testVoiceBtn.addEventListener('click', () => {
            const testUtterance = new SpeechSynthesisUtterance('Hello! This is a test of the selected voice settings.');
            testUtterance.rate = voiceRate;
            testUtterance.pitch = voicePitch;
            testUtterance.volume = voiceVolume;
            if (selectedVoice) testUtterance.voice = selectedVoice;
            synthesis.speak(testUtterance);
        });

        // Reset config button
        resetConfigBtn.addEventListener('click', () => {
            rateSlider.value = 1.2;  // Match default
            pitchSlider.value = 1.3; // Match default
            volumeSlider.value = 1.0;
            sendDelayInput.value = 600;
            maxWaitInput.value = 2000;
            minWordsInput.value = 1;
            echoThresholdInput.value = 20;
            maxTokensInput.value = 50;
            recognitionConfidenceInput.value = 0.3;
            interruptConfidenceInput.value = 0.2;

            // Trigger input events to update values
            rateSlider.dispatchEvent(new Event('input'));
            pitchSlider.dispatchEvent(new Event('input'));
            volumeSlider.dispatchEvent(new Event('input'));
            sendDelayInput.dispatchEvent(new Event('input'));
            maxWaitInput.dispatchEvent(new Event('input'));
            minWordsInput.dispatchEvent(new Event('input'));
            echoThresholdInput.dispatchEvent(new Event('input'));
            maxTokensInput.dispatchEvent(new Event('input'));
            recognitionConfidenceInput.dispatchEvent(new Event('input'));
            interruptConfidenceInput.dispatchEvent(new Event('input'));

            console.log('‚úÖ Config reset to default');
        });

        // Populate voice select
        function populateVoiceList() {
            if (!voiceSelect) {
                console.warn('Voice select element not found yet');
                return;
            }

            const voices = synthesis.getVoices();
            console.log(`üì¢ Populating voice list: ${voices.length} voices found`);

            voiceSelect.innerHTML = '';

            if (voices.length === 0) {
                const option = document.createElement('option');
                option.textContent = 'Loading voices...';
                voiceSelect.appendChild(option);
                return;
            }

            voices.forEach((voice, index) => {
                const option = document.createElement('option');
                option.value = index;
                option.textContent = `${voice.name} (${voice.lang})`;
                voiceSelect.appendChild(option);
            });

            if (voices.length > 0) {
                // Set default voice (index 6 or first available)
                const defaultIndex = voices.length > 6 ? 6 : 0;
                selectedVoice = voices[defaultIndex];
                voiceSelect.selectedIndex = defaultIndex; // Set dropdown to show selected voice
                console.log(`‚úÖ Default voice selected: ${selectedVoice.name} (index ${defaultIndex})`);
            }
        }

        // Load voices when ready
        if (synthesis.onvoiceschanged !== undefined) {
            synthesis.onvoiceschanged = () => {
                console.log('üîÑ Voices changed event triggered');
                populateVoiceList();
                listAvailableVoices();
            };
        }

        // Try to populate immediately
        setTimeout(() => {
            populateVoiceList();
            // If still no voices, try again after delay
            if (synthesis.getVoices().length === 0) {
                console.log('‚è≥ No voices yet, waiting for onvoiceschanged event...');
            }
        }, 100);

        // Voice interrupt only - no keyboard needed for natural conversation

        // Helper functions
        function safeStartRecognition(source = 'unknown') {
            // Check conditions BEFORE setting flag
            if (!recognition || !isListening) {
                console.log(`‚è∏Ô∏è Cannot start (recognition: ${!!recognition}, listening: ${isListening})`);
                return false;
            }

            if (isSpeaking || isProcessing) {
                console.log(`‚è∏Ô∏è AI-2 busy, skipping start from ${source}`);
                return false;
            }

            // Prevent multiple simultaneous starts
            if (isRestarting) {
                console.log(`‚è∏Ô∏è Already restarting, skipping start from ${source}`);
                return false;
            }

            try {
                isRestarting = true;

                // Clear any existing safety timeout
                if (restartingSafetyTimeout) {
                    clearTimeout(restartingSafetyTimeout);
                }

                // Safety timeout: Force reset flag after 2 seconds if still stuck
                restartingSafetyTimeout = setTimeout(() => {
                    if (isRestarting) {
                        console.warn('‚ö†Ô∏è SAFETY: Force resetting isRestarting flag (was stuck)');
                        isRestarting = false;
                    }
                }, 100);

                recognition.start();
                console.log(`‚úÖ Recognition started from ${source}`);

                // Reset flag after 500ms (recognition should have started by then)
                setTimeout(() => {
                    isRestarting = false;
                    console.log(`üîì isRestarting flag reset after ${source} start`);
                }, 500);

                return true;
            } catch (e) {
                console.warn(`‚ùå Start failed from ${source}:`, e.message);
                isRestarting = false;

                // Clear safety timeout on error
                if (restartingSafetyTimeout) {
                    clearTimeout(restartingSafetyTimeout);
                    restartingSafetyTimeout = null;
                }

                return false;
            }
        }

        function estimateTokenCount(text) {
            const words = text.trim().split(/\s+/).length;
            return Math.ceil(words / 0.75);
        }

        function updateStatus(message) {
            statusBar.textContent = message;
            statusBar.classList.remove('hidden');
        }

        function hideStatus() {
            setTimeout(() => statusBar.classList.add('hidden'), 3000);
        }

        // Check browser support
        if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
            alert('Speech recognition not supported. Please use Chrome or Edge.');
            startBtn.disabled = true;
        }

        async function startListening() {
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            recognition = new SpeechRecognition();

            // Get all audio input devices
            const devices = await navigator.mediaDevices.enumerateDevices();
            const audioInputs = devices.filter(device => device.kind === 'audioinput');

            console.log('üé§ Available audio inputs:', audioInputs.length);
            audioInputs.forEach((device, index) => {
                console.log(`  ${index}: ${device.label || 'Unknown Device'} (${device.deviceId.substring(0, 20)}...)`);
            });

            // Find external microphone (not "Built-in" or "Internal")
            let externalMic = audioInputs.find(device => {
                const label = device.label.toLowerCase();
                return !label.includes('built-in') &&
                    !label.includes('internal') &&
                    !label.includes('default') &&
                    device.deviceId !== 'default' &&
                    device.deviceId !== 'communications';
            });

            // If no external mic found, use first non-default device
            if (!externalMic && audioInputs.length > 1) {
                externalMic = audioInputs.find(device =>
                    device.deviceId !== 'default' &&
                    device.deviceId !== 'communications'
                );
            }

            const deviceId = externalMic ? externalMic.deviceId : undefined;

            if (externalMic) {
                console.log(`‚úÖ Using EXTERNAL microphone: ${externalMic.label}`);
            } else {
                console.warn('‚ö†Ô∏è No external microphone found, using default');
            }

            // Request microphone with AGGRESSIVE ECHO CANCELLATION for interrupt
            try {
                const stream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        deviceId: deviceId ? { exact: deviceId } : undefined, // Force external mic
                        echoCancellation: true,       // ENABLE for interrupt (filter speaker output)
                        noiseSuppression: true,       // ENABLE to filter AI voice
                        autoGainControl: true,        // Enable auto gain
                        sampleRate: 48000,            // Highest sample rate
                        channelCount: 1,
                        latency: 0,                   // Minimum latency
                        volume: 1.0                   // Maximum volume
                    }
                });

                // Create audio context for processing
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const source = audioContext.createMediaStreamSource(stream);

                // Add gain boost for user voice
                const gainNode = audioContext.createGain();
                gainNode.gain.value = 1.5; // Boost user voice 1.5x
                source.connect(gainNode);

                // Store for cleanup
                window.audioStream = stream;
                window.audioContext = audioContext;

                // Get actual device being used
                const track = stream.getAudioTracks()[0];
                const settings = track.getSettings();
                console.log('üé§ Microphone initialized with ECHO CANCELLATION for interrupt (gain: 1.5x)');
                console.log(`üîä Active device: ${track.label}`);
                console.log('üîä Echo cancellation: ENABLED - AI voice will be filtered from input');
            } catch (err) {
                console.warn('Could not set custom audio constraints:', err);
                // Fallback to basic settings with echo cancellation
                try {
                    const stream = await navigator.mediaDevices.getUserMedia({
                        audio: {
                            echoCancellation: true,
                            noiseSuppression: true
                        }
                    });
                    window.audioStream = stream;
                    console.log('üé§ Microphone initialized with basic echo cancellation');
                } catch (e) {
                    console.error('Microphone access failed:', e);
                }
            }

            // MAXIMUM sensitivity settings
            recognition.continuous = true;
            recognition.interimResults = true;
            recognition.lang = 'en-US';
            recognition.maxAlternatives = 10;  // Maximum alternatives for highest sensitivity

            recognition.onstart = () => {
                startBtn.disabled = true;
                stopBtn.disabled = false;
                ai1Circle.classList.add('active');
                updateStatus('Listening...');

                // Clear AI-2 text when user starts speaking
                aiResponseEl.textContent = 'Waiting for your message...';
                aiResponseEl.classList.add('empty');
            };

            recognition.onresult = (event) => {
                console.log('üé§ [RECOGNITION] Event triggered, resultIndex:', event.resultIndex, 'total results:', event.results.length);

                let interimTranscript = '';
                let finalTranscript = '';

                for (let i = event.resultIndex; i < event.results.length; i++) {
                    // Try all alternatives for maximum sensitivity
                    let bestTranscript = '';
                    let bestConfidence = 0;

                    console.log(`üîç [RECOGNITION] Processing result ${i}, alternatives:`, event.results[i].length);

                    // Check all alternatives and pick the best one
                    for (let j = 0; j < event.results[i].length; j++) {
                        const alt = event.results[i][j];
                        console.log(`   Alt ${j}: "${alt.transcript}" (conf: ${alt.confidence.toFixed(2)})`);
                        if (alt.confidence > bestConfidence || j === 0) {
                            bestTranscript = alt.transcript;
                            bestConfidence = alt.confidence;
                        }
                    }

                    const transcript = bestTranscript;
                    const confidence = bestConfidence;

                    if (event.results[i].isFinal) {
                        console.log(`üìù [FINAL] Best: "${transcript}" (conf: ${confidence.toFixed(2)}, threshold: ${RECOGNITION_CONFIDENCE_CONFIG})`);

                        // Check confidence threshold
                        if (confidence >= RECOGNITION_CONFIDENCE_CONFIG) {
                            finalTranscript += transcript;
                            console.log(`‚úÖ [ACCEPTED] Final transcript added:`, transcript);
                        } else {
                            console.log(`‚ùå [REJECTED] Low confidence (${confidence.toFixed(2)} < ${RECOGNITION_CONFIDENCE_CONFIG}), skipping:`, transcript);
                        }
                    } else {
                        interimTranscript += transcript;
                        console.log(`‚è≥ [INTERIM] "${transcript}" (conf: ${confidence.toFixed(2)})`);
                    }
                }

                // Show interim results
                if (interimTranscript) {
                    console.log(`üí¨ [INTERIM] Received: "${interimTranscript}"`);
                    console.log(`üìä [STATE] isSpeaking: ${isSpeaking}, isProcessing: ${isProcessing}`);

                    // ‚ö° INTERRUPT CHECK ON INTERIM (for real-time interrupt while AI speaking)
                    if (isSpeaking && ai2CurrentText && interimTranscript.trim().length > 0) {
                        console.log(`\nüîä [INTERIM INTERRUPT CHECK] ========== CHECKING INTERRUPT ==========`);
                        console.log(`üîä [INTERIM INTERRUPT] User said (interim): "${interimTranscript}"`);
                        console.log(`üîä [INTERIM INTERRUPT] AI-2 text: "${ai2CurrentText.substring(0, 50)}..."`);

                        // Filter echo from interim
                        const cleanedInterim = removeEchoFromInput(interimTranscript.trim(), ai2CurrentText);
                        console.log(`üßπ [INTERIM INTERRUPT] After echo filter: "${cleanedInterim}"`);

                        // Check if this is real user input (not echo)
                        // ALLOW 1 WORD for faster interrupt (e.g., "stop", "wait", "hey")
                        const isRealInput = cleanedInterim && cleanedInterim.trim().length > 0 &&
                            cleanedInterim.trim().split(/\s+/).length >= 1;

                        if (isRealInput) {
                            // Get confidence from interim results
                            let maxConfidence = 0;
                            for (let i = event.resultIndex; i < event.results.length; i++) {
                                if (!event.results[i].isFinal) {
                                    for (let j = 0; j < event.results[i].length; j++) {
                                        if (event.results[i][j].confidence > maxConfidence) {
                                            maxConfidence = event.results[i][j].confidence;
                                        }
                                    }
                                }
                            }

                            console.log(`üìä [INTERIM INTERRUPT] Confidence: ${maxConfidence.toFixed(2)}, Threshold: ${INTERRUPT_CONFIDENCE_CONFIG}`);

                            // Trigger interrupt if confidence is high enough
                            if (maxConfidence >= INTERRUPT_CONFIDENCE_CONFIG) {
                                console.log(`‚úÖ [INTERIM INTERRUPT] INTERRUPT TRIGGERED! Real input: "${cleanedInterim}"`);

                                // Set interrupt flag for graceful stop
                                shouldStopAfterSentence = true;
                                console.log('üõë [INTERIM INTERRUPT] Interrupt flag set');

                                // HYBRID APPROACH: Stop current utterance immediately, but gracefully
                                if (synthesis.speaking) {
                                    console.log('‚ö° [IMMEDIATE STOP] Stopping current speech chunk immediately');
                                    synthesis.cancel(); // Stop current chunk NOW

                                    // Reset speaking state
                                    isSpeaking = false;
                                    isProcessing = false;
                                    isSending = false;
                                    shouldStopAfterSentence = false; // Reset flag
                                    ai2Circle.classList.remove('active');

                                    if (sendQueue.length > 0) {
                                        console.log(`üóëÔ∏è [IMMEDIATE STOP] Clearing queue (${sendQueue.length} items)`);
                                        sendQueue = [];
                                    }

                                    aiResponseEl.textContent = 'Interrupted...';
                                    aiResponseEl.classList.add('empty');
                                    aiResponseEl.style.opacity = '0.5';
                                    ai2CurrentText = '';
                                    displayedAI2Words = [];

                                    console.log('‚úÖ [IMMEDIATE STOP] Speech stopped, ready for new input');

                                    // FORCE RESTART RECOGNITION after interrupt
                                    setTimeout(() => {
                                        if (isListening && recognition) {
                                            try {
                                                recognition.start();
                                                ai1Circle.classList.add('active');
                                                updateStatus('Listening...');
                                                console.log('üîÑ [FORCE RESTART] Recognition restarted after interrupt');
                                            } catch (e) {
                                                if (e.message.includes('already started')) {
                                                    console.log('‚úÖ [FORCE RESTART] Recognition already running');
                                                } else {
                                                    console.error('‚ùå [FORCE RESTART] Failed:', e.message);
                                                }
                                            }
                                        }
                                    }, 100);
                                } else {
                                    console.log('üõë [INTERIM INTERRUPT] No active speech, stopping immediately');
                                    isSpeaking = false;
                                    isProcessing = false;
                                    isSending = false;
                                    shouldStopAfterSentence = false;
                                    ai2Circle.classList.remove('active');

                                    if (sendQueue.length > 0) {
                                        console.log(`üóëÔ∏è [INTERIM INTERRUPT] Clearing queue (${sendQueue.length} items)`);
                                        sendQueue = [];
                                    }

                                    aiResponseEl.textContent = 'Interrupted...';
                                    aiResponseEl.classList.add('empty');
                                    aiResponseEl.style.opacity = '0.5';
                                    ai2CurrentText = '';
                                    displayedAI2Words = [];

                                    // FORCE RESTART RECOGNITION after interrupt
                                    setTimeout(() => {
                                        if (isListening && recognition) {
                                            try {
                                                recognition.start();
                                                ai1Circle.classList.add('active');
                                                updateStatus('Listening...');
                                                console.log('üîÑ [FORCE RESTART] Recognition restarted after interrupt');
                                            } catch (e) {
                                                if (e.message.includes('already started')) {
                                                    console.log('‚úÖ [FORCE RESTART] Recognition already running');
                                                } else {
                                                    console.error('‚ùå [FORCE RESTART] Failed:', e.message);
                                                }
                                            }
                                        }
                                    }, 100);
                                }
                            } else {
                                console.log(`‚ö†Ô∏è [INTERIM INTERRUPT] Confidence too low (${maxConfidence.toFixed(2)} < ${INTERRUPT_CONFIDENCE_CONFIG})`);
                            }
                        } else {
                            console.log(`üîá [INTERIM INTERRUPT] Echo or too short, ignoring`);
                        }
                    }

                    // Only show AI-1 input if not currently processing AI-2 response
                    if (!isSpeaking && !isProcessing) {
                        // FILTER ECHO from interim transcript before displaying
                        let cleanedInterim = interimTranscript;
                        if (ai2CurrentText && interimTranscript) {
                            console.log(`üîç [ECHO CHECK] Checking interim against AI-2: "${ai2CurrentText.substring(0, 50)}..."`);
                            cleanedInterim = removeEchoFromInput(interimTranscript, ai2CurrentText);
                            console.log(`üßπ [ECHO RESULT] Cleaned interim: "${cleanedInterim}"`);
                        }

                        // Only display if there's actual content after echo removal
                        if (cleanedInterim && cleanedInterim.trim().length > 0) {
                            const displayText = accumulatedTranscript + ' ' + cleanedInterim;
                            userTranscriptEl.textContent = displayText.trim();
                            userTranscriptEl.classList.remove('empty');
                            userTranscriptEl.style.opacity = '0.6';
                            console.log(`‚úÖ [DISPLAY] Showing interim: "${displayText.trim()}"`);

                            // ANIMATE AI-1 circle when detecting speech
                            ai1Circle.classList.add('active');
                        } else {
                            console.log(`‚è≠Ô∏è [SKIP] Interim empty after echo filter`);
                        }
                    } else {
                        console.log(`‚è∏Ô∏è [SKIP] Not displaying interim (AI-2 busy)`);
                        // Force clear AI-1 display if AI-2 is busy
                        if (userTranscriptEl.textContent !== 'Waiting for input...' &&
                            userTranscriptEl.textContent !== 'Sending...') {
                            console.log(`üßπ [FORCE CLEAR] Clearing AI-1 display (AI-2 busy)`);
                            userTranscriptEl.textContent = 'Waiting for input...';
                            userTranscriptEl.classList.add('empty');
                            userTranscriptEl.style.opacity = '1';
                        }
                    }

                    // Clear any pending send timeout while still speaking
                    if (sendTimeoutId) {
                        clearTimeout(sendTimeoutId);
                        sendTimeoutId = null;
                        console.log(`‚èπÔ∏è [TIMEOUT] Cleared pending send timeout`);
                    }
                }

                // Accumulate final results
                if (finalTranscript && finalTranscript.trim().length > 0) {
                    console.log(`\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ`);
                    console.log(`üéØ [FINAL TRANSCRIPT] Processing: "${finalTranscript}"`);
                    console.log(`üìä [STATE] isSpeaking: ${isSpeaking}, isProcessing: ${isProcessing}, isSending: ${isSending}`);
                    console.log(`üìö [ACCUMULATED] Current: "${accumulatedTranscript}"`);
                    console.log(`‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ`);

                    // ADVANCED ECHO DETECTION: Use node-edge matching
                    const transcriptLower = finalTranscript.trim().toLowerCase();

                    // DEBUG: Log interrupt check conditions
                    console.log(`üîç [INTERRUPT CHECK] isSpeaking=${isSpeaking}, ai2CurrentText="${ai2CurrentText ? ai2CurrentText.substring(0, 30) + '...' : 'EMPTY'}"`);

                    if (isSpeaking && ai2CurrentText) {
                        console.log(`\nüîä [AI-2 SPEAKING] ========== INTERRUPT CHECK ==========`);
                        console.log(`üîä [AI-2 SPEAKING] User said: "${finalTranscript}"`);
                        console.log(`üîä [AI-2 SPEAKING] AI-2 text: "${ai2CurrentText.substring(0, 50)}..."`);
                        console.log(`üîä [AI-2 SPEAKING] Checking if this is echo or real interrupt...`);

                        // Test if this is echo by trying to remove it
                        const cleanedText = removeEchoFromInput(finalTranscript.trim(), ai2CurrentText);
                        console.log(`üßπ [ECHO FILTER] After filter: "${cleanedText}"`);

                        // If 100% removed or very little left = ECHO
                        // ALLOW 1 WORD for faster interrupt
                        const isEcho = !cleanedText || cleanedText.trim().length === 0;

                        if (isEcho) {
                            console.log(`üîá [ECHO DETECTED] (node-edge match) - Ignoring: "${finalTranscript}"`);
                            return; // Ignore this input (it's echo)
                        }

                        console.log(`‚úÖ [NOT ECHO] Cleaned text has content: "${cleanedText}"`);

                        // Get confidence of this transcript for interrupt decision
                        let maxConfidence = 0;
                        for (let i = event.resultIndex; i < event.results.length; i++) {
                            if (event.results[i].isFinal) {
                                for (let j = 0; j < event.results[i].length; j++) {
                                    if (event.results[i][j].confidence > maxConfidence) {
                                        maxConfidence = event.results[i][j].confidence;
                                    }
                                }
                            }
                        }

                        // Check interrupt confidence threshold
                        if (maxConfidence < INTERRUPT_CONFIDENCE_CONFIG) {
                            console.log(`‚ö†Ô∏è Confidence too low for interrupt (${maxConfidence.toFixed(2)} < ${INTERRUPT_CONFIDENCE_CONFIG}), ignoring`);
                            return; // Don't interrupt on low confidence
                        }

                        // NOT echo + high confidence = real user input = INTERRUPT!
                        console.log(`‚úÖ USER INTERRUPT DETECTED - Real input: "${finalTranscript}" (cleaned: "${cleanedText}")`);

                        // Set interrupt flag
                        shouldStopAfterSentence = true;
                        console.log('üõë Interrupt flag set');

                        // HYBRID APPROACH: Stop current utterance immediately
                        if (synthesis.speaking) {
                            console.log('‚ö° [IMMEDIATE STOP] Stopping current speech chunk immediately');
                            synthesis.cancel(); // Stop current chunk NOW

                            // Reset speaking state
                            isSpeaking = false;
                            isProcessing = false;
                            isSending = false;
                            shouldStopAfterSentence = false; // Reset flag
                            ai2Circle.classList.remove('active');

                            if (sendQueue.length > 0) {
                                console.log(`üóëÔ∏è [IMMEDIATE STOP] Clearing queue (${sendQueue.length} items)`);
                                sendQueue = [];
                            }

                            aiResponseEl.textContent = 'Interrupted...';
                            aiResponseEl.classList.add('empty');
                            aiResponseEl.style.opacity = '0.5';
                            ai2CurrentText = '';
                            displayedAI2Words = [];

                            // Reset for new input
                            accumulatedTranscript = '';
                            firstSpeechTime = 0;

                            console.log('‚úÖ [IMMEDIATE STOP] Speech stopped, ready for new input');

                            // FORCE RESTART RECOGNITION after interrupt
                            setTimeout(() => {
                                if (isListening && recognition) {
                                    try {
                                        recognition.start();
                                        ai1Circle.classList.add('active');
                                        updateStatus('Listening...');
                                        console.log('üîÑ [FORCE RESTART] Recognition restarted after interrupt');
                                    } catch (e) {
                                        if (e.message.includes('already started')) {
                                            console.log('‚úÖ [FORCE RESTART] Recognition already running');
                                        } else {
                                            console.error('‚ùå [FORCE RESTART] Failed:', e.message);
                                        }
                                    }
                                }
                            }, 100);
                        } else {
                            console.log('üõë No active speech, stopping immediately');
                            isSpeaking = false;
                            isProcessing = false;
                            isSending = false;
                            shouldStopAfterSentence = false;
                            ai2Circle.classList.remove('active');

                            if (sendQueue.length > 0) {
                                console.log(`üóëÔ∏è Clearing queue (${sendQueue.length} items) due to voice interrupt`);
                                sendQueue = [];
                            }

                            aiResponseEl.textContent = 'Interrupted...';
                            aiResponseEl.classList.add('empty');
                            aiResponseEl.style.opacity = '0.5';
                            ai2CurrentText = '';

                            // Reset for new input
                            accumulatedTranscript = '';
                            firstSpeechTime = 0;

                            // Clear displayed words
                            displayedAI2Words = [];

                            // FORCE RESTART RECOGNITION after interrupt
                            setTimeout(() => {
                                if (isListening && recognition) {
                                    try {
                                        recognition.start();
                                        ai1Circle.classList.add('active');
                                        updateStatus('Listening...');
                                        console.log('üîÑ [FORCE RESTART] Recognition restarted after interrupt');
                                    } catch (e) {
                                        if (e.message.includes('already started')) {
                                            console.log('‚úÖ [FORCE RESTART] Recognition already running');
                                        } else {
                                            console.error('‚ùå [FORCE RESTART] Failed:', e.message);
                                        }
                                    }
                                }
                            }, 100);
                        }
                    }

                    // Track first speech time
                    if (!firstSpeechTime) {
                        firstSpeechTime = Date.now();
                        console.log('‚è±Ô∏è Started tracking speech time');
                    }

                    // Clear AI-2 when user speaks (reset AI-2 display)
                    if (!aiResponseEl.classList.contains('empty')) {
                        aiResponseEl.textContent = 'Waiting for your message...';
                        aiResponseEl.classList.add('empty');
                    }

                    // FILTER ECHO BEFORE DISPLAYING: Remove AI-2 echo using node-edge matching
                    const originalTranscript = finalTranscript.trim();
                    let cleanedTranscript = originalTranscript;

                    // Filter against current AI-2 text
                    if (ai2CurrentText) {
                        cleanedTranscript = removeEchoFromInput(cleanedTranscript, ai2CurrentText);
                    }

                    // Filter against recent AI-2 response history
                    for (const pastResponse of ai2ResponseHistory) {
                        if (cleanedTranscript && cleanedTranscript.trim().length > 0) {
                            cleanedTranscript = removeEchoFromInput(cleanedTranscript, pastResponse);
                        }
                    }

                    // Calculate how much was removed
                    const originalWordCount = originalTranscript.split(/\s+/).length;
                    const cleanedWordCount = cleanedTranscript ? cleanedTranscript.trim().split(/\s+/).filter(w => w.length > 0).length : 0;
                    const removalPercentage = originalWordCount > 0 ? ((originalWordCount - cleanedWordCount) / originalWordCount) * 100 : 0;

                    // If completely echo after all filtering, skip this transcript
                    if (!cleanedTranscript || cleanedTranscript.trim().length === 0) {
                        console.log(`üîá 100% echo in transcript (filtered), skipping: "${originalTranscript}"`);
                        return; // Don't add to accumulated, don't display
                    }

                    // ADDITIONAL CHECK: Simple substring matching for variations
                    // Check if cleaned transcript is a substring of any AI-2 response
                    const cleanedLower = cleanedTranscript.toLowerCase().replace(/[^\w\s]/g, '');
                    let isSubstringOfAI2 = false;

                    if (ai2CurrentText && ai2CurrentText.toLowerCase().replace(/[^\w\s]/g, '').includes(cleanedLower)) {
                        isSubstringOfAI2 = true;
                    }

                    if (!isSubstringOfAI2) {
                        for (const pastResponse of ai2ResponseHistory) {
                            if (pastResponse.toLowerCase().replace(/[^\w\s]/g, '').includes(cleanedLower)) {
                                isSubstringOfAI2 = true;
                                break;
                            }
                        }
                    }

                    if (isSubstringOfAI2) {
                        console.log(`üîá Substring match detected (echo variation): "${originalTranscript}" ‚Üí "${cleanedTranscript}"`);
                        return;
                    }

                    // If MORE THAN threshold was removed as echo, consider it ALL echo (likely just noise/partial echo)
                    if (removalPercentage > ECHO_THRESHOLD_CONFIG) {
                        console.log(`üîá ${removalPercentage.toFixed(0)}% echo detected (> ${ECHO_THRESHOLD_CONFIG}%), treating as full echo: "${originalTranscript}" ‚Üí "${cleanedTranscript}"`);
                        return;
                    }

                    // If too short after filtering, skip
                    // ALLOW 1 WORD for natural conversation (e.g., "yes", "no", "okay", "stop")
                    if (cleanedWordCount < 1 || !cleanedTranscript.trim()) {
                        console.log(`üîá Empty after echo filter, skipping: "${originalTranscript}"`);
                        return;
                    }

                    // Log if echo was removed
                    if (originalTranscript !== cleanedTranscript.trim()) {
                        console.log(`üßπ Echo filtered in transcript (${removalPercentage.toFixed(0)}% removed): "${originalTranscript}" ‚Üí "${cleanedTranscript}"`);
                    }

                    // Add cleaned transcript to accumulated
                    let newAccumulated = accumulatedTranscript + (accumulatedTranscript ? ' ' : '') + cleanedTranscript;

                    // Check token limit
                    const tokenCount = estimateTokenCount(newAccumulated);

                    if (tokenCount > MAX_TOKENS_CONFIG) {
                        console.log(`‚ö†Ô∏è Token limit reached: ${tokenCount}/${MAX_TOKENS_CONFIG}`);

                        // Send current accumulated before adding new
                        if (accumulatedTranscript.trim()) {
                            // DOUBLE CHECK: Filter echo before sending
                            let textToSend = accumulatedTranscript;
                            if (ai2CurrentText) {
                                textToSend = removeEchoFromInput(accumulatedTranscript, ai2CurrentText);
                            }

                            // Only send if not all echo
                            if (textToSend && textToSend.trim().length > 0 && textToSend.trim().split(/\s+/).length >= 2) {
                                accumulatedTranscript = '';

                                // Clear AI-1 display immediately
                                userTranscriptEl.textContent = 'Sending...';
                                userTranscriptEl.classList.add('empty');
                                userTranscriptEl.style.opacity = '0.6';

                                // Cancel any ongoing speech
                                if (isSpeaking) {
                                    synthesis.cancel();
                                    isSpeaking = false;
                                }

                                queueSendToAI(textToSend);
                            } else {
                                console.warn(`‚ö†Ô∏è [TOKEN LIMIT] Input RESET - echo detected`);
                                console.warn(`   Original: "${accumulatedTranscript}"`);
                                console.warn(`   After filter: "${textToSend}"`);
                                accumulatedTranscript = ''; // Clear echo

                                // Clear UI and restart recognition
                                userTranscriptEl.textContent = 'Waiting for input...';
                                userTranscriptEl.classList.add('empty');
                                updateStatus('Echo detected, waiting...');

                                // Recognition will auto-restart via onend (always active mode)
                            }
                        }

                        // Start new accumulation with current cleaned transcript
                        accumulatedTranscript = cleanedTranscript;
                    } else {
                        accumulatedTranscript = newAccumulated;
                    }

                    userTranscriptEl.textContent = accumulatedTranscript;
                    userTranscriptEl.classList.remove('empty');
                    userTranscriptEl.style.opacity = '1';

                    // ANIMATE AI-1 circle when speech is captured
                    ai1Circle.classList.add('active');

                    const currentTokens = estimateTokenCount(accumulatedTranscript);
                    const currentWords = accumulatedTranscript.trim().split(/\s+/).length;
                    console.log(`‚úÖ [ACCUMULATED] Text: "${accumulatedTranscript}"`);
                    console.log(`üìä [ACCUMULATED] Words: ${currentWords}, Tokens: ${currentTokens}`);

                    // Update last speech time
                    lastSendTime = Date.now();
                    console.log(`‚è∞ [TIMER] Last speech time updated: ${new Date(lastSendTime).toLocaleTimeString()}.${lastSendTime % 1000}`);

                    // Clear previous timeout
                    if (sendTimeoutId) {
                        clearTimeout(sendTimeoutId);
                    }

                    // Check if max wait time exceeded
                    const timeSinceFirstSpeech = Date.now() - firstSpeechTime;
                    if (timeSinceFirstSpeech >= MAX_WAIT_TIME_CONFIG && accumulatedTranscript.trim() && !isProcessing) {
                        console.log(`‚è∞ MAX WAIT TIME reached (${MAX_WAIT_TIME_CONFIG}ms) - force sending:`, accumulatedTranscript);

                        // DOUBLE CHECK: Filter echo one more time before sending
                        let textToSend = accumulatedTranscript;
                        if (ai2CurrentText) {
                            textToSend = removeEchoFromInput(accumulatedTranscript, ai2CurrentText);
                        }

                        // Skip if all echo after filtering
                        if (!textToSend || textToSend.trim().length === 0) {
                            console.warn(`‚ö†Ô∏è [MAX WAIT] Input RESET - 100% echo after filter`);
                            console.warn(`   Original: "${accumulatedTranscript}"`);
                            console.warn(`   After filter: "${textToSend}"`);
                            accumulatedTranscript = '';
                            firstSpeechTime = 0;
                            isProcessing = false;

                            // Clear UI and restart recognition
                            userTranscriptEl.textContent = 'Waiting for input...';
                            userTranscriptEl.classList.add('empty');
                            updateStatus('Echo detected, waiting...');

                            // Recognition will auto-restart via onend (always active mode)

                            return;
                        }

                        // Skip if empty after filtering (allow 1 word)
                        if (!textToSend || textToSend.trim().length === 0) {
                            console.log(`üîá Skipping send - empty after filter`);
                            accumulatedTranscript = '';
                            firstSpeechTime = 0;
                            isProcessing = false;

                            // Clear UI and restart recognition
                            userTranscriptEl.textContent = 'Waiting for input...';
                            userTranscriptEl.classList.add('empty');
                            updateStatus('Echo detected, waiting...');

                            // Recognition will auto-restart via onend (always active mode)

                            return;
                        }

                        // Force send immediately
                        isProcessing = true;
                        accumulatedTranscript = '';
                        firstSpeechTime = 0;

                        // Clear AI-1 display
                        userTranscriptEl.textContent = 'Sending...';
                        userTranscriptEl.classList.add('empty');

                        // Cancel any ongoing speech
                        if (isSpeaking) {
                            synthesis.cancel();
                            isSpeaking = false;
                        }

                        queueSendToAI(textToSend);
                        return; // Skip normal timeout logic
                    }

                    // BACKGROUND SEND: If >4 words and no change for 2 seconds, send to API in background
                    const wordCount = accumulatedTranscript.trim().split(/\s+/).length;
                    console.log(`üîç [BACKGROUND CHECK] Word count: ${wordCount}, Threshold: 4`);
                    if (wordCount >= 4 && !isProcessing && !isSpeaking) {
                        console.log(`‚è≤Ô∏è [BACKGROUND TIMER] Starting 2s timer for background send...`);
                        // Set background send timer (2 seconds)
                        setTimeout(() => {
                            const currentText = accumulatedTranscript.trim();
                            const currentWordCount = currentText.split(/\s+/).length;

                            console.log(`‚è∞ [BACKGROUND TIMER] 2s elapsed - checking conditions...`);
                            console.log(`   Current text: "${currentText}"`);
                            console.log(`   Current words: ${currentWordCount}`);
                            console.log(`   isProcessing: ${isProcessing}`);

                            // If still >=4 words and hasn't been sent yet, send in background
                            if (currentWordCount >= 4 && currentText && !isProcessing) {
                                console.log(`üì® [BACKGROUND TRIGGER] ‚úÖ Conditions met - sending context update`);

                                // Filter echo before background send
                                let textToSend = currentText;
                                if (ai2CurrentText) {
                                    console.log(`üîç [BACKGROUND FILTER] Checking echo against AI-2...`);
                                    textToSend = removeEchoFromInput(currentText, ai2CurrentText);
                                    console.log(`   After filter: "${textToSend}"`);
                                }

                                if (textToSend && textToSend.trim().length > 0) {
                                    console.log(`‚úÖ [BACKGROUND SEND] Calling sendToAPIBackground...`);
                                    sendToAPIBackground(textToSend);
                                    // Don't reset accumulator - let normal flow handle it
                                } else {
                                    console.log(`‚ùå [BACKGROUND SEND] Skipped - empty after filter`);
                                }
                            } else {
                                console.log(`‚è≠Ô∏è [BACKGROUND TIMER] Skipped - conditions not met`);
                            }
                        }, 2000); // 2 seconds for background send
                    } else {
                        console.log(`‚è≠Ô∏è [BACKGROUND CHECK] Skipped - not enough words or busy`);
                    }

                    // Wait for pause before sending (debounce)
                    console.log(`‚è≤Ô∏è [SEND TIMER] Starting ${SEND_DELAY_CONFIG}ms debounce timer...`);
                    sendTimeoutId = setTimeout(() => {
                        const timeSinceLastSpeech = Date.now() - lastSendTime;

                        console.log(`\n‚è∞ [SEND TIMER] ${SEND_DELAY_CONFIG}ms elapsed - checking conditions...`);
                        console.log(`   Time since last speech: ${timeSinceLastSpeech}ms`);
                        console.log(`   Accumulated: "${accumulatedTranscript}"`);
                        console.log(`   isProcessing: ${isProcessing}`);

                        // If no new speech for configured delay, send accumulated text
                        if (timeSinceLastSpeech >= SEND_DELAY_CONFIG && accumulatedTranscript.trim() && !isProcessing) {
                            console.log(`\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ`);
                            console.log(`üì§ [AUTO-SEND] ‚úÖ Conditions met - sending to API`);
                            console.log(`üì§ [AUTO-SEND] Text: "${accumulatedTranscript}"`);
                            console.log(`‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ`);

                            // DOUBLE CHECK: Filter echo one more time before sending
                            console.log(`üîç [FILTER] Checking echo before send...`);
                            let textToSend = accumulatedTranscript;
                            if (ai2CurrentText) {
                                console.log(`   AI-2 text: "${ai2CurrentText.substring(0, 50)}..."`);
                                textToSend = removeEchoFromInput(accumulatedTranscript, ai2CurrentText);
                                console.log(`   After filter: "${textToSend}"`);
                            } else {
                                console.log(`   No AI-2 text - no echo filter needed`);
                            }

                            // Skip if all echo after filtering
                            if (!textToSend || textToSend.trim().length === 0) {
                                console.warn(`\n‚ùå [AUTO-SEND] ABORTED - 100% echo after filter`);
                                console.warn(`   Original: "${accumulatedTranscript}"`);
                                console.warn(`   After filter: "${textToSend}"`);
                                console.warn(`   ‚ö†Ô∏è INPUT WILL BE RESET WITHOUT SENDING`);
                                accumulatedTranscript = '';
                                firstSpeechTime = 0;
                                isProcessing = false;
                                return;
                            }

                            // Skip if empty after filtering (allow 1 word)
                            if (!textToSend || textToSend.trim().length === 0) {
                                console.warn(`‚ö†Ô∏è [AUTO-SEND TIMEOUT] Input RESET - empty after filter`);
                                console.warn(`   Original: "${accumulatedTranscript}"`);
                                console.warn(`   After filter: "${textToSend}"`);
                                console.warn(`   Time since last speech: ${Date.now() - lastSendTime}ms`);
                                accumulatedTranscript = '';
                                firstSpeechTime = 0;
                                isProcessing = false;

                                // Clear UI and show waiting state
                                userTranscriptEl.textContent = 'Waiting for input...';
                                userTranscriptEl.classList.add('empty');
                                userTranscriptEl.style.opacity = '1';
                                updateStatus('Echo detected, waiting...');

                                // Recognition will auto-restart via onend (always active mode)

                                return;
                            }

                            // Prevent multiple sends
                            console.log(`üîí [AUTO-SEND] Setting isProcessing = true`);
                            isProcessing = true;

                            // Visual feedback: show sending state
                            userTranscriptEl.style.opacity = '0.6';
                            updateStatus('Sending to AI...');

                            // Cancel any ongoing speech
                            if (isSpeaking) {
                                console.log(`üõë [AUTO-SEND] Canceling ongoing AI speech`);
                                synthesis.cancel();
                                isSpeaking = false;
                            }

                            console.log(`\nüì§ [AUTO-SEND] ========== SENDING TO QUEUE ==========`);
                            console.log(`üì§ [AUTO-SEND] Text to send: "${textToSend}"`);
                            console.log(`üì§ [AUTO-SEND] Calling queueSendToAI()...`);

                            // Queue the send BEFORE resetting
                            queueSendToAI(textToSend);

                            console.log(`‚úÖ [AUTO-SEND] Queued successfully`);
                            console.log(`üîÑ [AUTO-SEND] Now resetting accumulator...`);

                            // Reset accumulator and timer AFTER queueing
                            accumulatedTranscript = '';
                            firstSpeechTime = 0;

                            // Clear AI-1 display immediately
                            userTranscriptEl.textContent = 'Sending...';
                            userTranscriptEl.classList.add('empty');

                            console.log(`‚úÖ [AUTO-SEND] Reset complete - accumulator cleared`);
                            console.log(`‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n`);
                        }
                    }, SEND_DELAY_CONFIG);
                }
            };

            recognition.onerror = (event) => {
                console.error(`‚ùå [RECOGNITION ERROR] ${event.error}`);

                // Log if error happens during AI speaking (interrupt attempt)
                if (isSpeaking) {
                    console.warn(`‚ö†Ô∏è [INTERRUPT BLOCKED] Recognition error while AI speaking: ${event.error}`);
                    console.warn(`üí° [TIP] Browser may be blocking recognition due to audio output`);
                }

                // Ignore these errors and keep listening
                if (event.error === 'no-speech' ||
                    event.error === 'audio-capture' ||
                    event.error === 'aborted') {
                    console.log('‚ö†Ô∏è Recoverable error, will restart...');

                    // Don't restart immediately - let onend handle it
                    return;
                }

                updateStatus('Error: ' + event.error);
                hideStatus();
            };

            recognition.onend = () => {
                if (DEBUG_RESET) {
                    console.log(`üîÑ [RECOGNITION END] isListening: ${isListening}, isSpeaking: ${isSpeaking}, isProcessing: ${isProcessing}, isZombieFixing: ${typeof isZombieFixing !== 'undefined' ? isZombieFixing : 'N/A'}`);
                }

                // SKIP auto-restart if zombie fix is in progress (prevent race condition)
                if (typeof isZombieFixing !== 'undefined' && isZombieFixing) {
                    if (DEBUG_RESET) console.log('‚è∏Ô∏è [AUTO-RESTART] Skipping auto-restart (zombie fix in progress)');
                    return;
                }

                // ALWAYS ACTIVE: Auto-restart immediately if still listening
                // EVEN IF AI-2 is speaking (for interrupt detection)
                if (isListening) {
                    if (DEBUG_RESET) console.log('üîÑ [AUTO-RESTART] Recognition ended, auto-restarting (always active mode)...');

                    // Restart IMMEDIATELY without delay
                    if (isListening) {
                        try {
                            recognition.start();
                            if (DEBUG_RESET) console.log('‚úÖ [AUTO-RESTART] Recognition restarted immediately');

                            // Log state for debugging
                            if (isSpeaking) {
                                console.log('üé§ [INTERRUPT MODE] Recognition active while AI-2 speaking - ready for interrupt!');
                            }
                        } catch (e) {
                            // If already started, that's fine - ignore error
                            if (e.message.includes('already started')) {
                                console.log('‚úÖ [AUTO-RESTART] Recognition already active');
                            } else {
                                console.warn('‚ö†Ô∏è [AUTO-RESTART] Restart error:', e.message);
                                // Try again after minimal delay
                                setTimeout(() => {
                                    if (isListening && (typeof isZombieFixing === 'undefined' || !isZombieFixing)) {
                                        try {
                                            recognition.start();
                                            console.log('‚úÖ [AUTO-RESTART] Recognition restarted (retry)');
                                        } catch (err) {
                                            console.error('‚ùå [AUTO-RESTART] Retry failed:', err.message);
                                        }
                                    }
                                }, 50); // Minimal 50ms delay for retry
                            }
                        }
                    }
                } else {
                    console.log('‚èπÔ∏è [RECOGNITION END] Not restarting (listening stopped)');
                }
            };

            // Set isListening BEFORE starting recognition
            isListening = true;
            lastUserInputTime = Date.now(); // Reset timer
            safeStartRecognition('initial');

            // Start heartbeat to ensure recognition stays alive
            let lastRecognitionEventTime = Date.now();
            let isZombieFixing = false; // Flag to prevent race condition

            // Track recognition events to detect zombie state
            const originalOnResult = recognition.onresult;
            recognition.onresult = (event) => {
                lastRecognitionEventTime = Date.now();
                originalOnResult(event);
            };

            recognitionHeartbeat = setInterval(() => {
                if (isListening) {
                    // Check if recognition is actually running
                    // console.log(`üíì [HEARTBEAT] Recognition check - isSpeaking: ${isSpeaking}, isProcessing: ${isProcessing}`);

                    // if (isSpeaking) {
                    //     console.log(`üé§ [HEARTBEAT] Recognition should be active for INTERRUPT while AI speaking`);
                    //     console.log(`üí° [TIP] Try saying "stop", "wait", or "hold on" to interrupt`);
                    // }

                    // ZOMBIE DETECTION: If no recognition events for 2 seconds while not speaking
                    const timeSinceLastEvent = Date.now() - lastRecognitionEventTime;
                    if (!isSpeaking && !isProcessing && !isZombieFixing && timeSinceLastEvent > 2000) {
                        // console.warn(`‚ö†Ô∏è [ZOMBIE DETECTED] No recognition events for ${Math.floor(timeSinceLastEvent / 1000)}s`);
                        // console.log('üîÑ [ZOMBIE FIX] Force stop + restart recognition...');

                        isZombieFixing = true; // Set flag to prevent race condition

                        try {
                            recognition.stop();
                            setTimeout(() => {
                                try {
                                    recognition.start();
                                    lastRecognitionEventTime = Date.now(); // Reset timer
                                    isZombieFixing = false; // Reset flag
                                    // console.log('‚úÖ [ZOMBIE FIX] Recognition restarted');
                                    ai1Circle.classList.add('active');
                                    updateStatus('Listening...');
                                } catch (e) {
                                    isZombieFixing = false; // Reset flag on error
                                    console.error('‚ùå [ZOMBIE FIX] Restart failed:', e.message);
                                }
                            }, 200); // Longer delay to ensure onend completes
                        } catch (e) {
                            isZombieFixing = false; // Reset flag on error
                            console.error('‚ùå [ZOMBIE FIX] Stop failed:', e.message);
                        }
                    }
                }
            }, 1000); // Check every 1 second

            // Silence nudge disabled - caused issues
            // startSilenceNudgeTimer();
        }

        function stopListening() {
            isListening = false;
            isProcessing = false; // Reset processing flag

            // Stop heartbeat
            if (recognitionHeartbeat) {
                clearInterval(recognitionHeartbeat);
                recognitionHeartbeat = null;
            }

            // Stop silence nudge timer
            stopSilenceNudgeTimer();

            if (recognition) {
                recognition.stop();
            }
            if (recognitionAI2) {
                recognitionAI2.stop();
            }
            synthesis.cancel();

            // Clear ALL pending timeouts
            if (sendTimeoutId) {
                clearTimeout(sendTimeoutId);
                sendTimeoutId = null;
            }
            if (restartTimeoutId) {
                clearTimeout(restartTimeoutId);
                restartTimeoutId = null;
            }

            // Cleanup audio resources
            if (window.audioContext) {
                window.audioContext.close();
                window.audioContext = null;
            }
            if (window.audioStream) {
                window.audioStream.getTracks().forEach(track => track.stop());
                window.audioStream = null;
            }

            accumulatedTranscript = '';
            ai1Circle.classList.remove('active');
            ai2Circle.classList.remove('active');

            startBtn.disabled = false;
            stopBtn.disabled = true;
            updateStatus('Stopped');
            hideStatus();
        }

        // Calculate text similarity (for echo detection)
        function calculateSimilarity(text1, text2) {
            if (!text1 || !text2) return 0;

            // Simple word-based similarity
            const words1 = text1.split(/\s+/);
            const words2 = text2.split(/\s+/);

            // Check how many words from text1 appear in text2
            let matchCount = 0;
            for (const word of words1) {
                if (word.length > 2 && text2.includes(word)) {
                    matchCount++;
                }
            }

            return words1.length > 0 ? matchCount / words1.length : 0;
        }

        // Advanced echo removal using node-edge graph matching
        // Removes subsequences that match AI-2's response path
        function removeEchoFromInput(userInput, ai2Response) {
            if (!userInput || !ai2Response) return userInput;

            // Normalize: lowercase and split into words (nodes)
            const userWords = userInput.toLowerCase().split(/\s+/).filter(w => w.length > 0);
            const ai2Words = ai2Response.toLowerCase().split(/\s+/).filter(w => w.length > 0);

            if (userWords.length === 0) return userInput;
            if (ai2Words.length === 0) return userInput;

            // Track matched paths with their lengths
            const matchedPaths = [];

            // Sliding window to find matching paths (edges)
            for (let ai2Start = 0; ai2Start < ai2Words.length; ai2Start++) {
                for (let userStart = 0; userStart < userWords.length; userStart++) {
                    let matchLength = 0;
                    let ai2Idx = ai2Start;
                    let userIdx = userStart;
                    const pathIndices = [];

                    // Try to match consecutive words (edge path)
                    while (ai2Idx < ai2Words.length && userIdx < userWords.length) {
                        if (ai2Words[ai2Idx] === userWords[userIdx]) {
                            pathIndices.push(userIdx);
                            matchLength++;
                            ai2Idx++;
                            userIdx++;
                        } else {
                            break;
                        }
                    }

                    // Only consider matches of 3+ consecutive words for strong echo detection
                    // OR 2 consecutive words if they're longer (not common words like "I am")
                    const isStrongMatch = matchLength >= 3 ||
                        (matchLength === 2 && pathIndices.every(idx => userWords[idx].length > 3));

                    if (isStrongMatch) {
                        matchedPaths.push({
                            indices: pathIndices,
                            length: matchLength,
                            start: userStart
                        });
                    }
                }
            }

            // Sort paths by length (longest first) to prioritize removing longer echo sequences
            matchedPaths.sort((a, b) => b.length - a.length);

            // Mark indices for removal, avoiding overlaps
            const matchedIndices = new Set();
            for (const path of matchedPaths) {
                // Check if this path overlaps with already marked indices
                const hasOverlap = path.indices.some(idx => matchedIndices.has(idx));
                if (!hasOverlap) {
                    // Add all indices from this path
                    path.indices.forEach(idx => matchedIndices.add(idx));
                }
            }

            // Build cleaned input by keeping only non-matched words
            const cleanedWords = [];
            for (let i = 0; i < userWords.length; i++) {
                if (!matchedIndices.has(i)) {
                    cleanedWords.push(userWords[i]);
                }
            }

            const cleanedText = cleanedWords.join(' ').trim();

            // Log if echo was removed
            if (matchedIndices.size > 0) {
                console.log(`üßπ Echo removed (${matchedPaths.length} paths, ${matchedIndices.size} words):`);
                console.log(`   Original: "${userInput}"`);
                console.log(`   Cleaned:  "${cleanedText}"`);
            }

            return cleanedText;
        }

        // Smart detection: Should AI respond verbally or just acknowledge silently?
        function shouldRespondVerbally(text, wordCount) {
            const lowerText = text.toLowerCase().trim();

            console.log(`ü§î [VERBAL CHECK] Input: "${text}" (${wordCount} words)`);

            // ONLY Filter: Very repetitive input (likely noise/testing)
            // Example: "test test test test" or "hello hello hello"
            const words = lowerText.split(/\s+/);
            const uniqueWords = new Set(words);
            if (words.length >= 3 && uniqueWords.size === 1) {
                console.log('‚ùå [NO AUDIO] Repetitive noise detected (same word repeated 3+ times)');
                return false;
            }

            // ALLOW EVERYTHING ELSE for natural conversation:
            // ‚úÖ Single words: "okay", "yes", "no", "hi", "bye"
            // ‚úÖ Filler words: "uh", "um", "hmm" (natural conversation)
            // ‚úÖ Short phrases: "I see", "got it", "oh really"
            // ‚úÖ Questions: "why?", "how?", "what?"
            console.log(`‚úÖ [AUDIO ON] Natural conversation input`);
            return true;
        }

        // Send nudge to keep conversation alive after silence
        function sendSilenceNudge() {
            if (!isListening || isSpeaking || isProcessing) {
                return; // Don't send if not listening or busy
            }

            const timeSinceLastInput = Date.now() - lastUserInputTime;
            if (timeSinceLastInput >= SILENCE_NUDGE_DELAY) {
                console.log('üí¨ Sending silence nudge after', Math.floor(timeSinceLastInput / 1000), 'seconds of silence');
                queueSendToAI('...', true); // Send ellipsis as nudge (bypass filters)
            }
        }

        // Start silence nudge timer
        function startSilenceNudgeTimer() {
            // Clear existing timer
            if (silenceNudgeTimer) {
                clearTimeout(silenceNudgeTimer);
            }

            // Set new timer - ONLY ONCE
            silenceNudgeTimer = setTimeout(() => {
                sendSilenceNudge();
                silenceNudgeTimer = null; // Clear after sending
                // DON'T auto-restart - will restart when user speaks next
            }, SILENCE_NUDGE_DELAY);
        }

        // Stop silence nudge timer
        function stopSilenceNudgeTimer() {
            if (silenceNudgeTimer) {
                clearTimeout(silenceNudgeTimer);
                silenceNudgeTimer = null;
            }
        }

        // Queue system with validation and deduplication
        function queueSendToAI(userText, isNudge = false) {
            console.log(`\nüì• [QUEUE] ========== QUEUE SEND TO AI ==========`);
            console.log(`üì• [QUEUE] Input: "${userText}"`);
            console.log(`üì• [QUEUE] Is nudge: ${isNudge}`);
            console.log(`üì• [QUEUE] Current queue size: ${sendQueue.length}`);

            // Validate input
            if (!userText || userText.trim().length === 0) {
                console.log('‚ùå [QUEUE] Empty input, skipping');
                return;
            }

            // BYPASS FILTERS for nudges (silence keepalive)
            if (isNudge) {
                console.log(`üí¨ [QUEUE] Sending nudge (bypassing filters): "${userText}"`);
                sendQueue.push(userText);
                processQueue();
                return;
            }

            // ADVANCED ECHO REMOVAL: Remove subsequences matching AI-2's responses
            // Uses node-edge graph matching to preserve word order
            // Check against current response AND recent history
            const originalText = userText;
            console.log(`üîç [ECHO FILTER] Original text: "${originalText}"`);
            console.log(`üîç [ECHO FILTER] AI-2 current: "${ai2CurrentText ? ai2CurrentText.substring(0, 50) + '...' : 'NONE'}"`);
            console.log(`üîç [ECHO FILTER] AI-2 history size: ${ai2ResponseHistory.length}`);

            // Filter against current AI-2 text
            if (ai2CurrentText) {
                const beforeFilter = userText;
                userText = removeEchoFromInput(userText, ai2CurrentText);
                console.log(`üßπ [ECHO FILTER] After current AI-2 filter: "${beforeFilter}" ‚Üí "${userText}"`);
            }

            // Filter against recent AI-2 response history (in case echo from previous responses)
            for (let i = 0; i < ai2ResponseHistory.length; i++) {
                if (userText && userText.trim().length > 0) {
                    const beforeFilter = userText;
                    userText = removeEchoFromInput(userText, ai2ResponseHistory[i]);
                    if (beforeFilter !== userText) {
                        console.log(`üßπ [ECHO FILTER] After history[${i}] filter: "${beforeFilter}" ‚Üí "${userText}"`);
                    }
                }
            }

            // If all words were removed (100% echo), skip
            if (!userText || userText.trim().length === 0) {
                console.log(`‚ùå [ECHO FILTER] 100% echo detected, SKIPPING: "${originalText}"`);
                return;
            }

            // If input became too short after echo removal, skip
            // ALLOW 1 WORD for natural conversation
            if (userText.trim().split(/\s+/).length < 1 || !userText.trim()) {
                console.log(`‚ùå [ECHO FILTER] Empty after filter, SKIPPING: "${originalText}" ‚Üí "${userText}"`);
                return;
            }

            // Log if echo was removed
            if (originalText !== userText) {
                console.log(`‚úÖ [ECHO FILTER] Echo removed: "${originalText}" ‚Üí "${userText}"`);
            } else {
                console.log(`‚úÖ [ECHO FILTER] No echo detected, text unchanged`);
            }

            // Check for duplicates in queue (prevent echo spam)
            const isDuplicate = sendQueue.some(item => item.toLowerCase() === userText.toLowerCase());
            if (isDuplicate) {
                console.log(`üîá Duplicate in queue, skipping: "${userText}"`);
                return;
            }

            // Check if too similar to last sent (prevent rapid duplicates)
            if (sendQueue.length > 0) {
                const lastInQueue = sendQueue[sendQueue.length - 1];
                const similarity = calculateSimilarity(userText.toLowerCase(), lastInQueue.toLowerCase());
                if (similarity > 0.8) {
                    console.log(`üîá Too similar to last queued (${(similarity * 100).toFixed(0)}%), skipping`);
                    return;
                }
            }

            console.log(`‚úÖ [QUEUE] Input ACCEPTED and queued: "${userText}"`);
            console.log(`üìä [QUEUE] New queue size: ${sendQueue.length + 1}`);
            sendQueue.push(userText);

            // Update last input time
            lastUserInputTime = Date.now();
            console.log(`‚è∞ [QUEUE] Last input time updated: ${new Date(lastUserInputTime).toLocaleTimeString()}`);

            console.log(`üîÑ [QUEUE] Calling processQueue()...`);
            processQueue();
        }

        async function processQueue() {
            console.log(`\n‚öôÔ∏è [PROCESS QUEUE] ========== PROCESSING QUEUE ==========`);
            console.log(`üìä [PROCESS QUEUE] Queue size: ${sendQueue.length}`);
            console.log(`üìä [PROCESS QUEUE] isSending: ${isSending}, isSpeaking: ${isSpeaking}, isProcessing: ${isProcessing}`);

            // If already processing, wait
            if (isSending) {
                console.log('‚è≥ [PROCESS QUEUE] Already sending, queued...');
                return;
            }

            // If AI-2 is speaking, wait
            if (isSpeaking) {
                console.log('‚è≥ [PROCESS QUEUE] AI-2 speaking, waiting...');
                return;
            }

            // If queue empty, done
            if (sendQueue.length === 0) {
                console.log('‚úÖ [PROCESS QUEUE] Queue empty, done');
                return;
            }

            // Get next item
            const userText = sendQueue.shift();
            console.log(`üì§ [PROCESS QUEUE] Dequeued: "${userText}"`);
            console.log(`üìä [PROCESS QUEUE] Remaining in queue: ${sendQueue.length}`);

            isSending = true;
            console.log(`üîí [PROCESS QUEUE] Set isSending = true`);

            try {
                console.log(`üöÄ [PROCESS QUEUE] Calling sendToAI()...`);
                await sendToAI(userText);
                console.log(`‚úÖ [PROCESS QUEUE] sendToAI() completed successfully`);
            } catch (error) {
                console.error('‚ùå [PROCESS QUEUE] Send error:', error);
                // CRITICAL: Reset flags on error to prevent freeze
                isProcessing = false;
                isSpeaking = false;
                ai2Circle.classList.remove('active');
                updateStatus('Error occurred, ready for input');
            } finally {
                isSending = false;
                console.log(`üîì [PROCESS QUEUE] Set isSending = false`);

                // Process next in queue after delay
                if (sendQueue.length > 0) {
                    console.log(`‚è≠Ô∏è [PROCESS QUEUE] Processing next item in 500ms...`);
                    setTimeout(processQueue, 500); // Longer delay for stability
                } else {
                    console.log(`‚úÖ [PROCESS QUEUE] All items processed`);
                }
            }
        }

        // Background send: Update context without waiting for response
        async function sendToAPIBackground(userText) {
            if (DEBUG_API) {
                console.log(`\nüì® [BACKGROUND SEND] ========== UPDATING CONTEXT ==========`);
                console.log(`üì® [BACKGROUND SEND] Input: "${userText}"`);
                console.log(`üì® [BACKGROUND SEND] Word count: ${userText.trim().split(/\s+/).length}`);
            }

            try {
                // Update conversation history
                conversationHistory.push({
                    role: 'user',
                    content: userText
                });

                if (conversationHistory.length > MAX_HISTORY * 2) {
                    conversationHistory = conversationHistory.slice(-MAX_HISTORY * 2);
                }

                // Send to API without waiting for response (fire and forget)
                fetch(`${backendUrlInput.value}/api/stream_chat`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        text: userText,
                        history: conversationHistory
                    })
                }).then(response => {
                    if (DEBUG_API) {
                        if (response.ok) {
                            console.log(`‚úÖ [BACKGROUND SEND] Context updated successfully`);
                        } else {
                            console.warn(`‚ö†Ô∏è [BACKGROUND SEND] Failed: ${response.status}`);
                        }
                    }
                }).catch(error => {
                    if (DEBUG_API) console.error(`‚ùå [BACKGROUND SEND] Error:`, error.message);
                });

                // Visual feedback
                updateStatus('Context updated (background)');
                setTimeout(() => {
                    if (isListening && !isSpeaking) {
                        updateStatus('Listening...');
                    }
                }, 1000);

            } catch (error) {
                console.error(`‚ùå [BACKGROUND SEND] Error:`, error);
            }
        }

        async function sendToAI(userText) {
            if (DEBUG_API) {
                console.log(`\nüöÄ [API CALL] ========== SENDING TO API ==========`);
                console.log(`üì§ [API CALL] Input text: "${userText}"`);
                console.log(`üìä [API CALL] History length: ${conversationHistory.length}`);
            }

            try {
                // Check if input needs verbal response (smart detection)
                const wordCount = userText.trim().split(/\s+/).length;
                const shouldSpeak = shouldRespondVerbally(userText, wordCount);

                console.log(`üìä [API CALL] Word count: ${wordCount}, Should speak: ${shouldSpeak}`);

                ai1Circle.classList.remove('active');

                // Only show AI-2 active if we'll speak
                if (shouldSpeak) {
                    ai2Circle.classList.add('active');
                    updateStatus('AI thinking...');
                } else {
                    updateStatus('Silent acknowledgment (context updated)');
                }

                conversationHistory.push({
                    role: 'user',
                    content: userText
                });

                if (conversationHistory.length > MAX_HISTORY * 2) {
                    conversationHistory = conversationHistory.slice(-MAX_HISTORY * 2);
                }

                // KEEP AI-1 text visible - don't clear it yet!
                // User should see what they said
                userTranscriptEl.style.opacity = '0.7'; // Dim it slightly

                // If input doesn't need verbal response, still send to API but no audio
                if (!shouldSpeak) {
                    console.log('‚è≠Ô∏è Silent mode - API called but no audio output');
                    aiResponseEl.textContent = '(Context updated silently...)';
                    aiResponseEl.classList.add('empty');
                    aiResponseEl.style.opacity = '0.5';

                    // Still update conversation history for context
                    conversationHistory.push({
                        role: 'user',
                        content: userText
                    });

                    if (conversationHistory.length > MAX_HISTORY * 2) {
                        conversationHistory = conversationHistory.slice(-MAX_HISTORY * 2);
                    }

                    isProcessing = false;

                    // Resume AI-1 recognition
                    setTimeout(() => {
                        if (isListening && recognition && !isProcessing) {
                            try {
                                recognition.start();
                                ai1Circle.classList.add('active');
                                updateStatus('Listening...');
                            } catch (e) {
                                console.warn('Resume failed:', e.message);
                            }
                        }
                    }, 300);
                    return;
                }

                // Prepare AI-2 display for streaming (don't clear, just prepare)
                aiResponseEl.classList.remove('empty');
                aiResponseEl.style.opacity = '1';

                // STREAMING: Get AI response with streaming
                console.log(`üåê [API CALL] Fetching: ${backendUrlInput.value}/api/stream_chat`);
                console.log(`üì¶ [API CALL] Payload:`, { text: userText, historyLength: conversationHistory.length });

                const response = await fetch(`${backendUrlInput.value}/api/stream_chat`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        text: userText,
                        history: conversationHistory
                    })
                });

                console.log(`üì° [API CALL] Response status: ${response.status}`);

                if (!response.ok) {
                    throw new Error(`AI request failed: ${response.status}`);
                }

                // KEEP AI-1 ACTIVE for voice interrupt detection
                // Echo will be filtered by strict similarity check
                console.log('üé§ [INTERRUPT READY] AI-1 stays active for voice interrupt detection');
                console.log(`üìä [STATE CHECK] isListening: ${isListening}, recognition exists: ${!!recognition}`);

                // FORCE ENSURE recognition is running for interrupt
                if (isListening && recognition) {
                    try {
                        // Try to start if not already running
                        recognition.start();
                        console.log('‚úÖ [INTERRUPT READY] Recognition force-started for interrupt detection');
                    } catch (e) {
                        if (e.message.includes('already started')) {
                            console.log('‚úÖ [INTERRUPT READY] Recognition already running - GOOD!');
                        } else {
                            console.error('‚ùå [INTERRUPT FAILED] Cannot start recognition:', e.message);
                        }
                    }
                } else {
                    console.warn('‚ö†Ô∏è [INTERRUPT DISABLED] Recognition not available!');
                }

                // NO AI-2 recognition - display text directly from streaming backend

                // Process streaming response
                const reader = response.body.getReader();
                const decoder = new TextDecoder();
                let fullText = '';
                let buffer = '';
                let sentenceBuffer = '';
                isSpeaking = true;
                console.log('üîä [AI-2 SPEAKING] Set isSpeaking = true');

                // Reset displayed words for new AI-2 response
                displayedAI2Words = [];

                // Clear AI-1 when AI-2 starts streaming
                console.log('üßπ [CLEAR AI-1] Clearing user input display');
                userTranscriptEl.textContent = 'Waiting for input...';
                userTranscriptEl.classList.add('empty');
                userTranscriptEl.style.opacity = '1';

                // Clear accumulated transcript to prevent it from showing again
                accumulatedTranscript = '';
                console.log('üßπ [CLEAR AI-1] Accumulated transcript cleared');

                // Clear AI-2 display for new response
                aiResponseEl.innerHTML = '';
                aiResponseEl.classList.remove('empty');

                updateStatus('AI speaking...');

                while (true) {
                    const { done, value } = await reader.read();

                    if (done) break;

                    buffer += decoder.decode(value, { stream: true });
                    const lines = buffer.split('\n');
                    buffer = lines.pop(); // Keep incomplete line in buffer

                    for (const line of lines) {
                        if (line.startsWith('data: ')) {
                            try {
                                const data = JSON.parse(line.slice(6));

                                if (data.token) {
                                    fullText += data.token;
                                    sentenceBuffer += data.token;

                                    // DON'T display text here - let word-by-word animation handle it
                                    // Just prepare AI-2 display area
                                    aiResponseEl.classList.remove('empty');
                                    aiResponseEl.style.opacity = '1';

                                    // Track AI-2 text for echo detection
                                    ai2CurrentText = fullText.trim().toLowerCase();

                                    // Speak when we have a complete sentence or phrase
                                    const hasEndPunctuation = /[.!?]$/.test(sentenceBuffer.trim());
                                    const hasComma = /,$/.test(sentenceBuffer.trim());
                                    const hasSemicolon = /[;:]$/.test(sentenceBuffer.trim());
                                    const hasEmDash = /[‚Äî‚Äì-]$/.test(sentenceBuffer.trim()); // Em dash, en dash, or hyphen at end
                                    const wordCount = sentenceBuffer.trim().split(/\s+/).length;

                                    // Speak if: sentence end, comma with 3+ words, semicolon, em dash, or 8+ words
                                    // REDUCED thresholds for faster interrupt response
                                    if (hasEndPunctuation || (hasComma && wordCount >= 3) || hasSemicolon || hasEmDash || wordCount >= 8) {
                                        const textToSpeak = sentenceBuffer.trim();
                                        // Skip if only punctuation (no actual words)
                                        const hasWords = /[a-zA-Z0-9]/.test(textToSpeak);
                                        if (textToSpeak && hasWords) {
                                            // Check for interrupt BEFORE speaking
                                            if (shouldStopAfterSentence) {
                                                console.log('üõë Interrupt detected, stopping before speaking chunk');
                                                sentenceBuffer = '';
                                                return; // Stop processing stream
                                            }
                                            speakChunk(textToSpeak);
                                            sentenceBuffer = '';
                                        } else if (textToSpeak) {
                                            console.log('‚è≠Ô∏è Skipping punctuation-only chunk:', textToSpeak);
                                            sentenceBuffer = '';
                                        }
                                    }
                                }

                                if (data.done) {
                                    originalAIText = data.full_text || fullText;

                                    // Speak any remaining text
                                    const remainingText = sentenceBuffer.trim();
                                    const hasWords = /[a-zA-Z0-9]/.test(remainingText);
                                    if (remainingText && hasWords) {
                                        speakChunk(remainingText);
                                    } else if (remainingText) {
                                        console.log('‚è≠Ô∏è Skipping punctuation-only remaining chunk:', remainingText);
                                    }

                                    // Update conversation history
                                    conversationHistory.push({
                                        role: 'assistant',
                                        content: originalAIText
                                    });

                                    // Save to AI-2 response history for extended echo filtering
                                    ai2ResponseHistory.push(originalAIText.toLowerCase());
                                    if (ai2ResponseHistory.length > MAX_AI2_HISTORY) {
                                        ai2ResponseHistory.shift(); // Remove oldest
                                    }

                                    console.log('‚úÖ Streaming complete:', originalAIText);
                                    console.log(`üìö AI-2 history size: ${ai2ResponseHistory.length}`);
                                }
                            } catch (e) {
                                console.warn('Parse error:', e);
                            }
                        }
                    }
                }

                // Wait for all speech to finish
                await waitForSpeechEnd();

                // No AI-2 recognition to stop

                isSpeaking = false;
                isProcessing = false;
                ai2Circle.classList.remove('active');
                // DON'T clear ai2CurrentText yet - keep it for echo filtering!

                // Process next in queue if any
                if (sendQueue.length > 0) {
                    console.log(`üìã Processing next in queue (${sendQueue.length} remaining)`);
                    setTimeout(processQueue, 300);
                }

                // AI-1 should already be running (kept active for voice interrupt)
                // Just ensure it's active and clear echo text after delay
                if (isListening) {
                    // Check if recognition is actually running by trying to access it
                    // If it's not running, we need to restart it
                    // Recognition is always active, just ensure UI is updated
                    ai1Circle.classList.add('active');
                    updateStatus('Listening...');
                    console.log('‚úÖ AI-1 remains active (always active mode)');

                    // DON'T clear ai2CurrentText yet - AI is still speaking!
                    // It will be cleared after all speech finishes (in waitForSpeechEnd callback)
                    console.log('‚è≥ Keeping ai2CurrentText for interrupt detection while speaking');

                    // Clear AI-2 display after short delay
                    setTimeout(() => {
                        if (!isSpeaking) { // Only clear if not speaking again
                            aiResponseEl.textContent = 'Waiting for your message...';
                            aiResponseEl.classList.add('empty');
                            aiResponseEl.style.opacity = '1';
                            console.log('üßπ Cleared AI-2 display');

                            // Force check recognition status
                            setTimeout(() => {
                                console.log(`üîç Checking recognition status: isListening=${isListening}, isSpeaking=${isSpeaking}, isProcessing=${isProcessing}`);

                                if (isListening && !isSpeaking && !isProcessing) {
                                    console.log('üîÑ Attempting to restart recognition...');
                                    // Try to trigger a restart if needed
                                    try {
                                        // This will fail if already running, which is fine
                                        recognition.start();
                                        console.log('‚úÖ Recognition restarted after AI-2');
                                    } catch (e) {
                                        if (e.message.includes('already started')) {
                                            console.log('‚ö†Ô∏è Recognition claims "already started" - might be zombie state');
                                            console.log('üîÑ Force stop + restart to fix zombie state...');

                                            // FORCE STOP + RESTART to fix zombie state
                                            try {
                                                recognition.stop();
                                                console.log('üõë Recognition stopped');

                                                // Wait a bit then restart
                                                setTimeout(() => {
                                                    try {
                                                        recognition.start();
                                                        console.log('‚úÖ Recognition force-restarted (zombie fix)');
                                                        ai1Circle.classList.add('active');
                                                        updateStatus('Listening...');
                                                    } catch (err) {
                                                        console.error('‚ùå Force restart failed:', err.message);
                                                    }
                                                }, 100);
                                            } catch (stopErr) {
                                                console.error('‚ùå Force stop failed:', stopErr.message);
                                            }
                                        } else {
                                            console.error('‚ùå Recognition restart FAILED:', e.message);
                                            // Force reset flags and try again
                                            isProcessing = false;
                                            isSpeaking = false;
                                            setTimeout(() => {
                                                try {
                                                    recognition.start();
                                                    console.log('‚úÖ Recognition restarted (retry after flag reset)');
                                                } catch (err) {
                                                    console.error('‚ùå Retry also failed:', err.message);
                                                }
                                            }, 200);
                                        }
                                    }
                                } else {
                                    console.warn('‚ö†Ô∏è Cannot restart - conditions not met');
                                }
                            }, 500);
                        }
                    }, 10); // Wait 1 second before clearing display
                } else {
                    console.log('‚ö†Ô∏è Not listening - isListening is false');
                }

            } catch (error) {
                console.error('Error:', error);
                updateStatus('Error: ' + error.message);
                aiResponseEl.textContent = 'Error: ' + error.message;
                aiResponseEl.classList.remove('empty');

                // Reset flags
                isSpeaking = false;
                isProcessing = false;
                ai2Circle.classList.remove('active');

                setTimeout(() => {
                    if (isListening) {
                        ai1Circle.classList.add('active');
                        updateStatus('Listening...');
                    }
                }, 100);
            }
        }

        // Get available voices (for debugging/selection)
        function listAvailableVoices() {
            const voices = synthesis.getVoices();
            console.log('üì¢ Available voices:', voices.length);
            voices.forEach((voice, index) => {
                console.log(`${index}: ${voice.name} (${voice.lang}) - ${voice.localService ? 'Local' : 'Remote'}`);
            });
            return voices;
        }

        // Load voices when ready
        if (synthesis.onvoiceschanged !== undefined) {
            synthesis.onvoiceschanged = listAvailableVoices;
        }

        // Speak a chunk of text with synchronized word-by-word display
        function speakChunk(text) {
            // Check if graceful interrupt was requested
            if (shouldStopAfterSentence) {
                console.log('üõë Skipping chunk due to graceful interrupt');
                return;
            }

            if (!text || text.trim().length === 0) return;

            // Remove emojis and emoticons before speaking
            const cleanText = removeEmojis(text);

            if (!cleanText || cleanText.trim().length === 0) return;

            const utterance = new SpeechSynthesisUtterance(cleanText);
            utterance.rate = voiceRate;     // From config slider
            utterance.pitch = voicePitch;   // From config slider
            utterance.volume = voiceVolume; // From config slider

            // Use selected voice from config
            if (selectedVoice) {
                utterance.voice = selectedVoice;
            }

            // Track current utterance
            currentUtterance = utterance;

            // Split into words for synchronized display
            const words = cleanText.split(' ');
            let currentWordIndex = 0;
            let displayedText = '';

            utterance.onstart = () => {
                console.log('üîä Speaking chunk:', cleanText);

                // Start word-by-word display
                displayWordsWithTiming(words);
            };

            utterance.onend = () => {
                console.log('‚úÖ Chunk finished speaking');

                // Check if graceful interrupt was requested during this chunk
                if (shouldStopAfterSentence) {
                    console.log('üõë Graceful interrupt: Stopped at phrase boundary (,‚Äî;:.!?)');

                    // Stop speaking and reset flags
                    synthesis.cancel();
                    isSpeaking = false;
                    isProcessing = false;
                    isSending = false;
                    shouldStopAfterSentence = false;
                    currentUtterance = null;
                    ai2Circle.classList.remove('active');

                    if (sendQueue.length > 0) {
                        console.log(`üóëÔ∏è Clearing queue (${sendQueue.length} items)`);
                        sendQueue = [];
                    }

                    aiResponseEl.textContent = 'Interrupted...';
                    aiResponseEl.classList.add('empty');
                    aiResponseEl.style.opacity = '0.5';
                    ai2CurrentText = '';

                    // Clear displayed words for next response
                    displayedAI2Words = [];
                }
            };

            utterance.onerror = (event) => {
                console.error('Speech error:', event);
                currentUtterance = null;
            };

            synthesis.speak(utterance);
        }

        // Display words with timing based on speech rate and punctuation
        function displayWordsWithTiming(words) {
            // Add new words to global array
            const startIndex = displayedAI2Words.length;
            displayedAI2Words.push(...words);

            let wordIndex = startIndex;

            function showNextWord() {
                if (wordIndex >= displayedAI2Words.length) return;
                if (!isSpeaking) return; // Stop if interrupted

                const word = displayedAI2Words[wordIndex];

                // Build HTML with all words, animating only the current one
                let html = '';
                for (let i = 0; i < displayedAI2Words.length; i++) {
                    if (i === wordIndex) {
                        // Current word with fade-in animation
                        html += `<span class="word-fade">${displayedAI2Words[i]}</span>`;
                    } else if (i < wordIndex) {
                        // Already displayed words (no animation)
                        html += displayedAI2Words[i];
                    }
                    // Don't show words after current index yet

                    if (i < wordIndex) html += ' ';
                }

                // Update display with animation
                aiResponseEl.innerHTML = html;
                aiResponseEl.scrollTop = aiResponseEl.scrollHeight;

                // Calculate delay based on word length and punctuation
                let delay = estimateWordDuration(word);

                // Check for punctuation at end of word
                const lastChar = word.slice(-1);
                const punctuationDelays = {
                    '.': 400,  // Period: longer pause
                    '!': 400,  // Exclamation
                    '?': 400,  // Question
                    ',': 250,  // Comma: medium pause
                    ';': 300,  // Semicolon
                    ':': 300   // Colon
                };

                if (punctuationDelays[lastChar]) {
                    delay += punctuationDelays[lastChar];
                }

                wordIndex++;
                setTimeout(showNextWord, delay);
            }

            showNextWord();
        }

        // Estimate word duration based on syllables, word type, and natural speech patterns
        function estimateWordDuration(word) {
            const cleanWord = word.toLowerCase().replace(/[^a-z]/g, '');

            // Very short function words (articles, pronouns, etc.) - FAST
            const quickWords = ['a', 'an', 'the', 'i', 'you', 'he', 'she', 'it', 'we', 'they',
                'am', 'is', 'are', 'was', 'were', 'be', 'been',
                'to', 'of', 'in', 'on', 'at', 'by', 'for', 'with'];

            if (quickWords.includes(cleanWord)) {
                return 120; // Very fast (120ms)
            }

            // Conjunctions and short connectors - FAST
            const connectors = ['and', 'but', 'or', 'so', 'if', 'as', 'than', 'that', 'this'];
            if (connectors.includes(cleanWord)) {
                return 140; // Fast (140ms)
            }

            // Common short words - MEDIUM-FAST
            if (cleanWord.length <= 3) {
                return 160; // Medium-fast
            }

            // Count syllables for longer words
            const syllables = countSyllables(cleanWord);

            // Base timing: At rate 1.3, approximately 4.5 syllables per second
            const syllablesPerSecond = 4.5;
            let duration = (syllables / syllablesPerSecond) * 1000;

            // Adjust for word complexity
            // Longer words (7+ letters) get slightly more time for clarity
            if (cleanWord.length >= 7) {
                duration *= 1.1; // 10% slower for complex words
            }

            // Very long words (10+ letters) even more time
            if (cleanWord.length >= 10) {
                duration *= 1.15; // 15% slower for very complex words
            }

            return Math.max(150, Math.min(duration, 600)); // Min 150ms, max 600ms
        }

        // Count syllables in a word
        function countSyllables(word) {
            word = word.toLowerCase().replace(/[^a-z]/g, '');
            if (word.length <= 3) return 1;

            // Count vowel groups
            const vowels = word.match(/[aeiouy]+/g);
            let count = vowels ? vowels.length : 1;

            // Adjust for silent e
            if (word.endsWith('e')) count--;

            return Math.max(1, count);
        }

        // Remove emojis and emoticons from text
        function removeEmojis(text) {
            // Remove emoji unicode characters
            text = text.replace(/[\u{1F600}-\u{1F64F}]/gu, ''); // Emoticons
            text = text.replace(/[\u{1F300}-\u{1F5FF}]/gu, ''); // Symbols & Pictographs
            text = text.replace(/[\u{1F680}-\u{1F6FF}]/gu, ''); // Transport & Map
            text = text.replace(/[\u{1F1E0}-\u{1F1FF}]/gu, ''); // Flags
            text = text.replace(/[\u{2600}-\u{26FF}]/gu, '');   // Misc symbols
            text = text.replace(/[\u{2700}-\u{27BF}]/gu, '');   // Dingbats
            text = text.replace(/[\u{FE00}-\u{FE0F}]/gu, '');   // Variation Selectors
            text = text.replace(/[\u{1F900}-\u{1F9FF}]/gu, ''); // Supplemental Symbols
            text = text.replace(/[\u{1FA70}-\u{1FAFF}]/gu, ''); // Symbols and Pictographs Extended-A

            // Remove text emoticons
            text = text.replace(/[:;=xX][oO\-]?[D\)\]\(\[pP\/\\OpP3]/g, '');
            text = text.replace(/[oO][_\-]?[oO]/g, '');
            text = text.replace(/[><!][_\-]?[<>]/g, '');

            // Clean up extra spaces
            text = text.replace(/\s+/g, ' ').trim();

            return text;
        }

        // Wait for all queued speech to finish
        function waitForSpeechEnd() {
            return new Promise((resolve) => {
                const checkInterval = setInterval(() => {
                    if (!synthesis.speaking && !synthesis.pending) {
                        clearInterval(checkInterval);

                        // NOW clear ai2CurrentText - all speech finished
                        ai2CurrentText = '';
                        console.log('üßπ Cleared ai2CurrentText - all speech finished');

                        resolve();
                    }
                }, 100);
            });
        }

        // Start speech recognition for AI-2 to capture spoken text
        function startAI2Recognition() {
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            recognitionAI2 = new SpeechRecognition();

            recognitionAI2.continuous = true;
            recognitionAI2.interimResults = true;
            recognitionAI2.lang = 'en-US';
            recognitionAI2.maxAlternatives = 5;

            let ai2CurrentSentence = ''; // Only show current sentence

            recognitionAI2.onstart = () => {
                console.log('üé§ AI-2 recognition started - capturing speech');
            };

            recognitionAI2.onresult = (event) => {
                let interimTranscript = '';
                let finalTranscript = '';

                for (let i = event.resultIndex; i < event.results.length; i++) {
                    const transcript = event.results[i][0].transcript;

                    if (event.results[i].isFinal) {
                        finalTranscript += transcript;
                    } else {
                        interimTranscript += transcript;
                    }
                }

                // Show interim results with lower opacity
                if (interimTranscript) {
                    const displayText = ai2CurrentSentence + ' ' + interimTranscript;
                    aiResponseEl.textContent = displayText.trim();
                    aiResponseEl.style.opacity = '0.7';
                    aiResponseEl.scrollTop = aiResponseEl.scrollHeight;
                }

                // Accumulate final results
                if (finalTranscript && finalTranscript.trim().length > 0) {
                    ai2CurrentSentence += (ai2CurrentSentence ? ' ' : '') + finalTranscript.trim();

                    // Check if sentence ended (period, exclamation, question mark)
                    if (/[.!?]$/.test(ai2CurrentSentence.trim())) {
                        console.log('‚úÖ Sentence complete:', ai2CurrentSentence);
                        // Clear for next sentence
                        ai2CurrentSentence = '';
                        aiResponseEl.textContent = '...'; // Show transition
                        aiResponseEl.style.opacity = '0.5';
                    } else {
                        // Show current sentence
                        aiResponseEl.textContent = ai2CurrentSentence;
                        aiResponseEl.style.opacity = '1';
                    }

                    aiResponseEl.scrollTop = aiResponseEl.scrollHeight;
                    console.log('üìù AI-2 captured:', finalTranscript);
                }
            };

            recognitionAI2.onerror = (event) => {
                console.warn('AI-2 recognition error:', event.error);
                // Ignore recoverable errors
                if (event.error === 'no-speech' || event.error === 'aborted') {
                    return;
                }
            };

            recognitionAI2.onend = () => {
                // Auto-restart if still speaking
                if (isSpeaking && recognitionAI2) {
                    try {
                        recognitionAI2.start();
                        console.log('üîÑ AI-2 recognition restarted');
                    } catch (e) {
                        console.warn('AI-2 restart failed:', e.message);
                    }
                } else {
                    console.log('üîá AI-2 recognition ended');
                }
            };

            try {
                recognitionAI2.start();
            } catch (e) {
                console.error('Failed to start AI-2 recognition:', e);
            }
        }

        // Stop AI-2 recognition
        function stopAI2Recognition() {
            if (recognitionAI2) {
                recognitionAI2.stop();
                recognitionAI2 = null;
                console.log('üõë AI-2 recognition stopped');
            }
        }





        async function clearHistory() {
            try {
                await fetch(`${backendUrlInput.value}/api/history`, {
                    method: 'DELETE'
                });

                conversationHistory = [];
                userTranscriptEl.textContent = 'Waiting for input...';
                userTranscriptEl.classList.add('empty');
                aiResponseEl.textContent = 'Waiting for your message...';
                aiResponseEl.classList.add('empty');

                updateStatus('History cleared');
                hideStatus();

            } catch (error) {
                console.error('Error clearing history:', error);
            }
        }

        // Initialize
        updateStatus('Ready');
        hideStatus();
    </script>
</body>

</html>